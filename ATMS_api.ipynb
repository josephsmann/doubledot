{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doubledot.ATMS\n",
    "\n",
    ">  Definition of the ATMS_api class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ATMS_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from nbdev.showdoc import *\n",
    "import requests\n",
    "import json\n",
    "import jmespath as jp\n",
    "import re\n",
    "from time import sleep\n",
    "from fastcore.basics import patch\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "from fastcore.test import test_eq\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from pyisemail import is_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ATMS_api:\n",
    "    class_download_dir = os.path.join(os.getcwd(),'atms_download')\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_all_data():\n",
    "        \"\"\" delete all the subdirectories and files in the download directory \"\"\"\n",
    "        for dir_path in glob.glob(os.path.join(ATMS_api.class_download_dir, \"*\")):\n",
    "            if os.path.isdir(dir_path):\n",
    "                for file_path in glob.glob(os.path.join(dir_path, \"*\")):\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"File '{file_path}' deleted successfully.\")\n",
    "                os.removedirs(dir_path) \n",
    "                print(f\"Directory '{dir_path}' deleted successfully.\")  \n",
    "        print(f\"Directory '{ATMS_api.class_download_dir}' deleted successfully.\")   \n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod    \n",
    "    def get_atms_authentication():\n",
    "        \"\"\"get access token for ATMS API\n",
    "\n",
    "        Returns:\n",
    "            response object: response object from the API call \n",
    "        \"\"\"\n",
    "        vantix_url = \"http://crm-api-telus.atmsplus.com/auth\"\n",
    "        \n",
    "        with open('secrets.json') as f:\n",
    "            secrets = json.load(f)\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"username\": secrets['vantix_user'],\n",
    "            \"password\": secrets['vantix_pw'],\n",
    "            \"rememberMe\": True\n",
    "        })\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", vantix_url, headers=headers, data=payload)\n",
    "        assert response.status_code == 200, f\"response code is {response.status_code}, not 200\"\n",
    "        return response.json().get('access_token')\n",
    "\n",
    "                \n",
    "    #make a function that takes a string and returns a string with the original semi-colon separated emails replaced with a list of emails with quotes around each\n",
    "\n",
    "    @staticmethod\n",
    "    def _mutate_email_list(s:str # a string like: ' ... \"emails\": [ ... \"address\": \"pam.jenkins@blackgold.ca;don.what@this.com\"}], ...  '\n",
    "                            ) -> str : # the same string but with ' ... \"address\": [\"pam.jenkins@blackgold.ca\", \"don.what@this.com\"]} ...\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        pat_s = r\"\"\"\\\"emails\\\": \\[.*?address\\\": (\\\"(?P<email1>.*?)\\\")\"\"\"\n",
    "        pattern=re.compile(pat_s)\n",
    "        matches = re.search(pattern,s) \n",
    "        if matches and matches.group(0) and matches.group(1) and matches.group(2):\n",
    "            og_emails_list_s = matches.group(2)\n",
    "            # emails_l = [f\"\\\"{email}\\\"\" for email in og_emails_list_s.split(';')]\n",
    "            emails_l = [f\"\\\"{email}\\\"\" for email in og_emails_list_s.split(';') if is_email(email)]\n",
    "            # emails_l = [f\"{email}\" for email in og_emails_list_s.split(';')]\n",
    "            emails_list_s = '[' +','.join(emails_l)+']'\n",
    "            return re.sub(matches.group(1), emails_list_s,s)\n",
    "        else:\n",
    "            return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __init__(self:ATMS_api):\n",
    "    self.telus_access_token = ATMS_api.get_atms_authentication()\n",
    "    self.obj_d = {}\n",
    "\n",
    "    # create unique download directory per instance\n",
    "    if not os.path.exists(ATMS_api.class_download_dir):\n",
    "        os.makedirs(ATMS_api.class_download_dir)\n",
    "        print(f\"Directory 'atms_download' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory 'atms_download' already exists.\")\n",
    "\n",
    "    # generate a unique directory name with a random string\n",
    "    random_string = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))\n",
    "    self.download_dir  = os.path.join(ATMS_api.class_download_dir, random_string)\n",
    "\n",
    "    # Check if the new director already exists in the directory\n",
    "    while os.path.exists(self.download_dir):\n",
    "        # If the directory name already exists, generate a new one\n",
    "        random_string  = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))\n",
    "        self.download_dir  = os.path.join(ATMS_api.class_download_dir, random_string)\n",
    "    \n",
    "    os.mkdir(self.download_dir)\n",
    "    self.id = random_string\n",
    "    print(\"my id is\", self.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch    \n",
    "def list_files(self:ATMS_api):\n",
    "    \"\"\" returns list of files in download folder \"\"\"\n",
    "    print('id is:', self.id)\n",
    "    if os.path.exists(self.download_dir):\n",
    "        return os.listdir(self.download_dir)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "@patch\n",
    "def clean_data_dir(self:ATMS_api,\n",
    "                    obj_s: str = None):\n",
    "    glob_s = os.path.join(self.download_dir, f\"*{obj_s if obj_s else ''}*.json\")\n",
    "    file_l = glob.glob(glob_s)\n",
    "    # print(file_l)\n",
    "    for file_path in file_l:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"File '{file_path}' deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "@patch\n",
    "def get_telus_data(self:ATMS_api, \n",
    "                    obj: str, # telus endpoint \n",
    "                    offset :int = 0, # the row to begin retrieval\n",
    "                    since_date: str = \"\", # optionally, the date from which to retrieve\n",
    "                    count :int =1000 # the number of rows to retrieve\n",
    "                    ):\n",
    "    \"\"\"retrieve data from ATMS API, should be private method\n",
    "\n",
    "    Args:\n",
    "        obj (string): api endpoint to retrieve data from\n",
    "        offset (int, optional): first row to begin retrieval. Defaults to 0.\n",
    "        count (int, optional): number of rows to retrieve. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        a dict with two keys: \"response\" and \"done\". \n",
    "        \"response\" is the response object from the API call. \"done\" is a boolean indicating whether there are more records to retrieve.\n",
    "    \"\"\"\n",
    "    vantix_data_url = f\"http://crm-api-telus.atmsplus.com/api/{obj}?offset={offset}&count={count}\"\n",
    "    if len(since_date)> 0:\n",
    "        print(\"ATMS_api.get_telus_data: since_date is\", since_date)\n",
    "        vantix_data_url = f\"http://crm-api-telus.atmsplus.com/api/{obj}/lastupdate?count={count}&offset={offset}&updateDate={since_date}\"\n",
    "\n",
    "    v_headers = {'Authorization': f\"Bearer {self.telus_access_token}\"}\n",
    "\n",
    "    print(vantix_data_url)    \n",
    "    # response = requests.request(\"GET\", vantix_data_url, headers=v_headers, data={}).json()\n",
    "    response = requests.request(\"GET\", vantix_data_url, headers=v_headers, data={})\n",
    "    \n",
    "    # inform caller we're done if we get fewer records than requested\n",
    "    # BUT there could still be an error in the response\n",
    "    return {\"response\": response, \"done\":  response.status_code!=200 or len(response.json()) < count}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "@patch\n",
    "def retrieve_and_clean(self:ATMS_api, \n",
    "                        obj : str = 'contacts', # ATMS object to retrieve\n",
    "                        initial_offset : int =0, # start retrieval at row initial_offset\n",
    "                        rows_per_batch :int =1000, # number records retrieved at once\n",
    "                        since_date : str = \"\", # if given, it will be used instead of `initial_offset` \n",
    "                        max_rows :int = 0 # maximum number of rows to retrieve\n",
    "                        ):\n",
    "    \"\"\"Retrieve data from ATMS API, clean data and write to file\"\"\"\n",
    "    self.write_obj_to_file(obj, initial_offset, rows_per_batch, since_date, max_rows)\n",
    "    self.clean_data_file(obj)\n",
    "    self.load_data_file_to_dict(obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "@patch\n",
    "def write_obj_to_file(self:ATMS_api, \n",
    "                        obj : str = 'contacts', # ATMS object to retrieve\n",
    "                        initial_offset : int =0, # start retrieval at row initial_offset\n",
    "                        rows_per_batch :int =1000, # number records retrieved at once\n",
    "                        since_date : str = \"\", # if given, it will be used instead of `initial_offset` \n",
    "                        max_rows :int = 0 # maximum number of rows to retrieve\n",
    "                        ):\n",
    "    \"\"\"Retrieve data from ATMS API and write to file\n",
    "        private method\n",
    "\n",
    "    Args:\n",
    "        obj (string): a valid ATMS REST API object\n",
    "        rows_per_batch (int, optional): maximum number of rows to retieve. Defaults to 1000.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    # offset is the starting row for the next batch\n",
    "    offset = initial_offset \n",
    "\n",
    "    filename_s = f'atms_{obj}.json'\n",
    "    print('download dir is: ', self.download_dir)\n",
    "    file_path_s = os.path.join(self.download_dir, filename_s)\n",
    "    # print(\"Writing to file: \", file_path_s)\n",
    "    \n",
    "    # if max_rows is 0, we'll retrieve all rows\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        f.write(\"[ \\n\")\n",
    "        num_rows_for_next_batch = min(rows_per_batch, max_rows ) if max_rows > 0 else rows_per_batch\n",
    "\n",
    "        #max_remaining_rows will be decremented as we retrieve batches\n",
    "        max_remaining_rows = max_rows if max_rows > 0 else rows_per_batch\n",
    "\n",
    "        #num_rows_for_next_batch will be \n",
    "        num_rows_for_next_batch = min(rows_per_batch, max_remaining_rows) \n",
    "        first_line = True\n",
    "        while (not done and (num_rows_for_next_batch > 0)):\n",
    "\n",
    "            # read another batch\n",
    "            \n",
    "            print(f\"resp_d = self.get_telus_data({obj},offset={offset}, count= {num_rows_for_next_batch}, since_date={since_date})\")\n",
    "            resp_d = self.get_telus_data(obj,offset=offset, count= num_rows_for_next_batch, since_date=since_date)\n",
    "            print(\"resp_d.keys(): \", resp_d.keys())\n",
    "            if resp_d['response'].status_code != 200:\n",
    "                print(f\"Error retrieving data: {resp_d['response'].status_code}\")\n",
    "                print(f\"Error retrieving data: {resp_d['response'].json()}\")\n",
    "                raise ValueError(f\"Error retrieving data: {resp_d['response'].status_code}\")\n",
    "            obj_l = resp_d['response'].json()\n",
    "            done = resp_d['done'] ###### if done is set to True, we'll exit the loop\n",
    "            print(f\"done: {done}, resp_d.json() : {resp_d['response']}\")\n",
    "            \n",
    "            # assert len(obj_l)>0, f\"response: {resp_d}\"\n",
    "            try:\n",
    "                s = json.dumps(obj_l[0])\n",
    "            except:\n",
    "                print(\"Error converting to json: resp_d \", resp_d.json())\n",
    "                print(\"Error converting to json: obj_l \", obj_l)\n",
    "                raise ValueError(f\"Error converting to json {obj_l}\")\n",
    "            for o in obj_l[1:]:\n",
    "                try:\n",
    "                    s += ',\\n' +json.dumps(o) \n",
    "                except:\n",
    "                    print(\"Error converting to json: \", obj_l)\n",
    "                    raise ValueError(f\"Error converting to json {o}\")\n",
    "            # s = \",\\n\".join([json.dumps(o) for o in obj_l])\n",
    "            \n",
    "            if first_line:\n",
    "                f.write(s)\n",
    "                first_line = False\n",
    "            else:\n",
    "                f.write(\",\\n\"+s)\n",
    "            offset += rows_per_batch\n",
    "            max_remaining_rows = max_rows - (offset - initial_offset) if max_rows > 0 else rows_per_batch\n",
    "            num_rows_for_next_batch = min(rows_per_batch, max_remaining_rows)\n",
    "        f.write(\"\\n]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_data_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clean_data_file(self:ATMS_api, \n",
    "                    obj_s : str\n",
    "                    ): \n",
    "    \"\"\" Massage formatting to work with Salesfore Bulk API\n",
    "    \n",
    "    \"\"\"\n",
    "    # read original contacts file   \n",
    "    in_filename_s = f'atms_{obj_s}.json'\n",
    "    in_file_path_s = os.path.join(self.download_dir, in_filename_s)\n",
    "    print(\"cleaning_data_file - download dir is: \", self.download_dir)\n",
    "    # print(\"Cleaning file: \", in_file_path_s)\n",
    "    try:\n",
    "        with open(in_file_path_s,'r') as f:\n",
    "            # write modified contacts file \n",
    "            out_filename_s = f'atms_transformed_{obj_s}.json'\n",
    "            out_file_path_s = os.path.join(self.download_dir, out_filename_s)\n",
    "            print(\"creating file: \", out_file_path_s)\n",
    "            with open(out_file_path_s,'w') as f2:\n",
    "                s = f.read()\n",
    "                for l in s.split('\\n'):\n",
    "                    # remove \"O`Brien\" problem\n",
    "                    l2 = re.sub('\\u2019',\"'\",l)\n",
    "                    # fix emails\n",
    "                    new_s = ATMS_api._mutate_email_list(l2)+'\\n'\n",
    "                    f2.write(new_s)\n",
    "            print(f\"Finished cleaning {in_filename_s} -> {out_file_path_s}\")\n",
    "    except FileNotFoundError:\n",
    "        print('the files in our download dir:', os.listdir(self.download_dir) )\n",
    "        print('our in_file_path: ', in_file_path_s)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_data_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load_data_file_to_dict(\n",
    "        self:ATMS_api,\n",
    "        obj_s : str # ATMS object. eg. contacts|items|memberships|membership\n",
    "        ):\n",
    "        \"\"\" `load_data_file_to_dict` will attempt to load a cleaned json file into a dict for future parsing. \n",
    "        If the cleaned file doesn't exist, it will look for a dirty one to clean.\n",
    "        If the dirty once doesn't exist, it will raise an exception. It won't be downloaded because we don't know how much to get.\n",
    "        \n",
    "        \"\"\"\n",
    "        file_name_s = f'atms_transformed_{obj_s}.json'\n",
    "        file_path_s = os.path.join(self.download_dir, file_name_s)\n",
    "        print('ATMS_api - Attempting to load: ', file_path_s, ' into dict')\n",
    "        data=\"\"\n",
    "        try:\n",
    "            with open(file_path_s, 'r') as f:\n",
    "                data = f.read()\n",
    "        except FileNotFoundError:  # clean file not found\n",
    "            print(\"File not found. Check that the dirty file is there\")\n",
    "            dirty_file_name_s = f'atms_{obj_s}.json'\n",
    "            dirty_file_path_s = os.path.join(self.download_dir, dirty_file_name_s)\n",
    "\n",
    "            if os.path.exists(dirty_file_path_s):\n",
    "                print(f\"found dirty file: {dirty_file_path_s}\")\n",
    "                self.clean_data_file(obj_s)\n",
    "                with open(file_path_s,'r') as f:\n",
    "                    data = f.read()\n",
    "            else:\n",
    "                raise FileNotFoundError # we give up\n",
    "        finally: # this will be executed regardless of first 'try' failing or not\n",
    "            if len(data) > 0:\n",
    "                try: # data might be buggy\n",
    "                    self.obj_d[obj_s] = json.loads(data)\n",
    "                    print(f\"ATMS_api: loaded {len(self.obj_d[obj_s])} {obj_s} into dict\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"We've got buggy data, or something\")\n",
    "                    raise Exception('Data is not JSON formatted')\n",
    "            # assert obj_s in self.obj_d, f\" '{obj_s}' not in {self.obj_d.keys()}\"\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pick list for MembershipTerm.memberType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch \n",
    "def get_pick_list_for_MembeshipTerms_membershipType( self: ATMS_api):\n",
    "    \"\"\" utility func to get picklist values for 'membershipTyp'\n",
    "    this needs a full download to work\"\"\"\n",
    "    self.retrieve_and_clean('memberships')\n",
    "    print('\\n'.join(set(jp.search(\"[].membershipTerms[].membershipType\", self.obj_d['memberships']))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'atms_download' already exists.\n",
      "my id is h0eomo8d\n"
     ]
    }
   ],
   "source": [
    "atms = ATMS_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "resp_d = self.get_telus_data(contacts,offset=0, count= 1000, since_date=2020-01-01)\n",
      "ATMS_api.get_telus_data: since_date is 2020-01-01\n",
      "http://crm-api-telus.atmsplus.com/api/contacts/lastupdate?count=1000&offset=0&updateDate=2020-01-01\n",
      "resp_d.keys():  dict_keys(['response', 'done'])\n",
      "done: False, resp_d.json() : <Response [200]>\n",
      "cleaning_data_file - download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "creating file:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_contacts.json\n",
      "Finished cleaning atms_contacts.json -> /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_contacts.json\n",
      "ATMS_api - Attempting to load:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_contacts.json  into dict\n",
      "ATMS_api: loaded 1000 contacts into dict\n",
      "download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "resp_d = self.get_telus_data(memberships,offset=0, count= 1000, since_date=2020-01-01)\n",
      "ATMS_api.get_telus_data: since_date is 2020-01-01\n",
      "http://crm-api-telus.atmsplus.com/api/memberships/lastupdate?count=1000&offset=0&updateDate=2020-01-01\n",
      "resp_d.keys():  dict_keys(['response', 'done'])\n",
      "done: False, resp_d.json() : <Response [200]>\n",
      "cleaning_data_file - download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "creating file:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_memberships.json\n",
      "Finished cleaning atms_memberships.json -> /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_memberships.json\n",
      "ATMS_api - Attempting to load:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_memberships.json  into dict\n",
      "ATMS_api: loaded 1000 memberships into dict\n",
      "download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "resp_d = self.get_telus_data(items,offset=0, count= 1000, since_date=2020-01-01)\n",
      "ATMS_api.get_telus_data: since_date is 2020-01-01\n",
      "http://crm-api-telus.atmsplus.com/api/items/lastupdate?count=1000&offset=0&updateDate=2020-01-01\n",
      "resp_d.keys():  dict_keys(['response', 'done'])\n",
      "done: False, resp_d.json() : <Response [200]>\n",
      "cleaning_data_file - download dir is:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d\n",
      "creating file:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_items.json\n",
      "Finished cleaning atms_items.json -> /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_items.json\n",
      "ATMS_api - Attempting to load:  /Users/josephmann/Documents/Github/doubledot/atms_download/h0eomo8d/atms_transformed_items.json  into dict\n",
      "ATMS_api: loaded 1000 items into dict\n"
     ]
    }
   ],
   "source": [
    "atms.retrieve_and_clean('contacts', max_rows=1000, since_date='2020-01-01')\n",
    "atms.retrieve_and_clean('memberships', max_rows=1000, since_date='2020-01-01')\n",
    "atms.retrieve_and_clean('items', max_rows=1000, since_date='2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\"playtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.delete_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATMS_api.delete_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.get_telus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.write_obj_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.load_data_file_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.clean_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ATMS_api.clean_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vantix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
