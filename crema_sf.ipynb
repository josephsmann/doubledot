{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doubledot.Salesforce\n",
    "> Salesforce class for transfering data from Vantix to Salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp crema_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "import io\n",
    "from nbdev.showdoc import *\n",
    "import requests\n",
    "import json\n",
    "import jmespath as jp\n",
    "import re\n",
    "from time import sleep\n",
    "from fastcore.basics import patch\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "from doubledot.ATMS_api import ATMS_api as ATMS\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salesforce @property sf_access_token @staticmethod list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/josephsmann/doubledot/blob/master/doubledot/crema_sf.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Salesforce.sf_access_token\n",
       "\n",
       ">      Salesforce.sf_access_token ()\n",
       "\n",
       "a @property\n",
       "retrieve token for Salesforce - verifies that token is still valid and attempts to get a new one from Salesforce site if not"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/josephsmann/doubledot/blob/master/doubledot/crema_sf.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Salesforce.sf_access_token\n",
       "\n",
       ">      Salesforce.sf_access_token ()\n",
       "\n",
       "a @property\n",
       "retrieve token for Salesforce - verifies that token is still valid and attempts to get a new one from Salesforce site if not"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "## Module for Salesforce API\n",
    "\n",
    "class Salesforce:\n",
    "    \"\"\"Class for Salesforce API\"\"\"\n",
    "    class_download_dir = os.path.join(os.getcwd(),'sf_download')\n",
    "    class_upload_dir = os.path.join(os.getcwd(),'sf_upload')\n",
    "    transfer_lock = False # lock to prevent multiple transfers at once - not implemented yet. but probably necessary to work with withh nbdev_test \n",
    "\n",
    "    ## Salesforce table relationships \n",
    "    ## these are also orderd by dependency\n",
    "    model_d = {     'Contact'               :{ 'lookups_d': dict(), 'external_id':'contactId__c'},\n",
    "                    'Membership__c'         :{ 'lookups_d': dict(), 'external_id':'membershipId__c'},\n",
    "                    'MembershipTerm__c'     :{ 'lookups_d': {'membershipKey__c': 'Membership__c'}, 'external_id':'membershipTermId__c'},\n",
    "                    'MembershipMember__c'   :{ 'lookups_d': {'contactKey__c': 'Contact', 'membershipTermKey__c': 'MembershipTerm__c' }, 'external_id':'membershipMemberId__c'},\n",
    "                    # in SF change saleId__c to saleId__c\n",
    "                    'Sale__c'               :{ 'lookups_d': {'booking_contactKey__c': 'Contact'}, 'external_id':'saleId__c'},\n",
    "                    # in SF change membershipTermKey__c to membershipTermKey__c\n",
    "                    'SaleDetail__c'         :{ 'lookups_d': {'membershipTermKey__c':'MembershipTerm__c', 'saleKey__c': 'Sale__c'}, 'external_id':'saleDetailId__c'},\n",
    "                    # 'SaleDetail__c'         :{ 'lookups_d': {'membershipTermKey__c':'MembershipTerm__c', 'saleId__c': 'Sale__c'}, 'external_id':'saleDetailId__c'},\n",
    "                    # in SF change tickeKey__c to ticketId__c\n",
    "                    # change saleId__c to saleKey__c in Ticket__c\n",
    "                    'Ticket__c'             :{ 'lookups_d': {'saleKey__c': 'Sale__c', 'saleDetailKey__c': 'SaleDetail__c'}, 'external_id':'ticketId__c'} \n",
    "            }\n",
    "\n",
    "    def __init__(self):\n",
    "        # set up access token \n",
    "        self._sf_access_token = self.get_token_with_REST()\n",
    "        self.bulk_job_id = None\n",
    "        self._atms = None\n",
    "\n",
    "        # create unique download directory per instance\n",
    "        if not os.path.exists(Salesforce.class_download_dir):\n",
    "            os.makedirs(Salesforce.class_download_dir)\n",
    "            print(f\"Directory 'atms_download' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory 'atms_download' already exists.\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sf_access_token(\n",
    "        self \n",
    "     ) -> str : #the access toke\n",
    "        \"\"\"a @property\n",
    "        retrieve token for Salesforce - verifies that token is still valid and attempts to get a new one from Salesforce site if not\n",
    "        \"\"\"\n",
    "        if not(self.test_token()):\n",
    "            self._sf_access_token = self.get_token_with_REST()\n",
    "            # check to see if getting token worked\n",
    "            assert (self.sf_access_token), \"Fetching new token didn't fix problem\"\n",
    "        return self._sf_access_token\n",
    "\n",
    "\n",
    "    @property\n",
    "    def atms( self):\n",
    "        if not self._atms:\n",
    "            self._atms = ATMS()\n",
    "        return self._atms\n",
    "    \n",
    "    @staticmethod\n",
    "    def list_files():\n",
    "        return os.listdir(Salesforce.class_download_dir)\n",
    "\n",
    "show_doc(Salesforce.sf_access_token)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_token_with_REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_with_REST(self: Salesforce):\n",
    "    \"\"\"retieve the access token from Salesforce\n",
    "\n",
    "    Returns:\n",
    "        string: the access token \n",
    "    \"\"\"\n",
    "    with open('secrets.json') as f:\n",
    "        secrets = json.load(f)\n",
    "    \n",
    "    DOMAIN = secrets['instance']\n",
    "    payload = {\n",
    "        'grant_type': 'password',\n",
    "        'client_id': secrets['client_id'],\n",
    "        'client_secret': secrets['client_secret'],\n",
    "        'username': secrets['username'],\n",
    "        'password': secrets['password'] + secrets['security_token']\n",
    "    }\n",
    "    oauth_url = f'{DOMAIN}/services/oauth2/token'\n",
    "\n",
    "    auth_response = requests.post(oauth_url, data=payload)\n",
    "    return auth_response.json().get('access_token') ######## <<<<<<<<<<<<<<<< .       \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test_token(self: Salesforce):\n",
    "    \"\"\"Verify that token is still valid. If it isn't, it attempts to get a new one.\n",
    "\n",
    "    Returns:\n",
    "        boolean: true if token is valid, false otherwise\n",
    "    \"\"\"\n",
    "    sf_headers = { 'Authorization': f\"Bearer {self._sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    end_point =\"https://cremaconsulting-dev-ed.develop.my.salesforce.com\"\n",
    "    service = \"/services/data/v57.0/\"\n",
    "    r = requests.request(\"GET\", end_point+service+f\"limits\", headers=sf_headers, data={})\n",
    "    valid_token = r.status_code == 200\n",
    "    if not(valid_token): print(r.status_code, type(r.status_code))\n",
    "    return valid_token\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def create_job(self: Salesforce, \n",
    "                sf_object: str ='Contact', # the Salesforce object were going to operate on. \n",
    "                operation: str ='insert', # the database operation to use. Can be \"insert\",\"upsert\" or \"delete\"\n",
    "                external_id: str = 'External_Id__c' # when using \"upsert\", this field is used to identify the record\n",
    "                )-> requests.Response :\n",
    "    \"\"\"Get job_id from Salesforce Bulk API\n",
    "\n",
    "    \"\"\"\n",
    "    # Args: \n",
    "    #     sf_object (str, optional): the Salesforce object were going to operate on. Defaults to 'Contact'.\n",
    "    #     operation (str, optional): âˆ†. Defaults to 'insert'.\n",
    "    #     external_id (str, optional): the external id field for upsert operations. Defaults to 'External_Id__c'.\n",
    "    #     sf_object (str, optional): the Salesforce object were going to operate on. Defaults to 'Contact'.\n",
    "    #     operation (str, optional): the operation that will be used against the object. Defaults to 'insert'.\n",
    "    #     external_id (str, optional): the external id field for upsert operations. Defaults to 'External_Id__c'.\n",
    "    #     contentType (str, optional): the content type of the file. Defaults to 'CSV', 'JSON' also accepted.\n",
    "    # Returns: \n",
    "    #     response: a response object containg the job_id. For more information on the response object see https://www.w3schools.com/python/ref_requests_response.asp\n",
    "    #     a response object see https://www.w3schools.com/python/ref_requests_response.asp\n",
    "        \n",
    "    # Salesforce API docs: https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/create_job.htm    \n",
    "    url = \"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest\"\n",
    "\n",
    "    # https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/datafiles_prepare_csv.htm\n",
    "    ## we can set columnDelimiter to `,^,|,;,<tab>, and the default <comma>\n",
    "    # sets the object to Contact, the content type to CSV, and the operation to insert\n",
    "    payload_d = {\n",
    "        \"object\": sf_object,\n",
    "        \"contentType\": \"CSV\",\n",
    "        # set columnDelimiter to TAB instead of comma for ease of dealing with commas in address fields\n",
    "        #https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/create_job.htm\n",
    "        \"columnDelimiter\": \"TAB\", \n",
    "        \"operation\": operation\n",
    "    }\n",
    "    \n",
    "    # as per https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/walkthrough_upsert.htm\n",
    "    if operation=='upsert':\n",
    "        payload_d['externalIdFieldName']=external_id\n",
    "    payload = json.dumps(payload_d)\n",
    "    \n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    # print(response.text)\n",
    "    try:\n",
    "        self.bulk_job_id = response.json()['id']\n",
    "    except TypeError:\n",
    "        print(\"Bad response in Salesforce.create_job :\\n\", response.text)\n",
    "        \n",
    "    print(f\"Created job {self.bulk_job_id} for {sf_object} with operation {operation}\") \n",
    "    return response \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def upload_csv(self : Salesforce, \n",
    "                obj_s: str = \"\", # Salesforce object to upload \n",
    "                use_download_dir_b : bool = False, # \n",
    "                # num_rows: int = 100, # the number of rows to upload \n",
    "                ) -> requests.Response:\n",
    "    \"\"\"Using the job_id from the previous step, upload the csv file to the job\n",
    "\n",
    "    Args:\n",
    "        file (filepointer): file pointer to the csv filek\n",
    "    \"\"\"\n",
    "    # if not(file):\n",
    "    #     # throw error\n",
    "    #     assert False, \"File not found\"\n",
    "\n",
    "\n",
    "    assert obj_s in ['Contact', 'Membership__c', 'MembershipTerm__c', 'MembershipMember__c', 'Sale__c', 'Ticket__c', 'SaleDetail__c']\n",
    "\n",
    "    print(f\"Uploading job {self.bulk_job_id} of object {obj_s}\")\n",
    "\n",
    "    if use_download_dir_b:\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir , f\"{obj_s}.csv\")\n",
    "    else:\n",
    "        file_path_s = os.path.join(Salesforce.class_upload_dir , f\"{obj_s}.csv\")\n",
    "\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/batches\"\n",
    "\n",
    "    # replace all occurrences of '\\2019' with \\'\n",
    "    # we may have done this in ATMS already, but just in case\n",
    "    try:\n",
    "        for line in fileinput.input(files=file_path_s, inplace=True):\n",
    "            line = line.replace('\\u2019', \"'\")\n",
    "            print(line, end='') # this prints to the file instead of stdout (!!)\n",
    "\n",
    "        with open(file_path_s,'r') as payload:\n",
    "            headers = {\n",
    "                'Content-Type': 'text/csv',\n",
    "                'Authorization': f'Bearer {self.sf_access_token}'\n",
    "                }\n",
    "            response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found error in Saleforce.upload_csv: \", file_path_s)\n",
    "        return None\n",
    "    \n",
    "    return response\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## close_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def close_job(self: Salesforce):\n",
    "    # close the job (from Postman)\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"state\": \"UploadComplete\"\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"PATCH\", url, headers=headers, data=payload)\n",
    "\n",
    "    # print(response.text)\n",
    "    return response.json()\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## job_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export       \n",
    "# get job status (from Postman)\n",
    "@patch\n",
    "def job_status(self: Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## successful_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def successful_results(self : Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/successfulResults\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def failed_results(self: Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/failedResults\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    try:\n",
    "        display(pd.DataFrame(response.json()))\n",
    "    except:\n",
    "        print(response.text)\n",
    "    # \n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_sf_object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_sf_object_ids(self: Salesforce, \n",
    "                      object: str = 'Contact' # REST endpoint for data object\n",
    "                      ):\n",
    "    \"\"\"Get Safesforce IDs for a the specified object\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Retrieving Object Ids for {object} from Salesforce\")\n",
    "    sf_headers = { 'Authorization': f\"Bearer {self.sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    end_point =\"https://cremaconsulting-dev-ed.develop.my.salesforce.com\"\n",
    "    service = \"/services/data/v57.0/\"\n",
    "    r = requests.request(\"GET\", end_point+service+f\"query/?q=SELECT+Id+FROM+{object}\", headers=sf_headers, data={})\n",
    "    assert isinstance(r.json(), dict), f\"response: {r.json()}, header: {sf_headers}\"\n",
    "    object_ids = [d.get('Id') for d in r.json()['records']]\n",
    "    while r.json()['done'] == False:\n",
    "        new_url = end_point+r.json()['nextRecordsUrl']\n",
    "        print(new_url)\n",
    "        r = requests.request(\"GET\", new_url, headers=sf_headers, data={})\n",
    "        print((r.json()))\n",
    "        fresh_object_ids = [d.get('Id') for d in r.json()['records']]\n",
    "        print(len(fresh_object_ids))   \n",
    "        object_ids+=fresh_object_ids\n",
    "        \n",
    "    print('total number of objects = ',len(object_ids))\n",
    "    return object_ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_sf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def delete_sf_objects(self: Salesforce, \n",
    "                      obj_l: str | list = []\n",
    "                      ):\n",
    "    \"\"\"Delete Salesforce objects from the specified list or string. Empty list deletes all objects.\"\"\"\n",
    "    # assert False, \"want to catch test calls to this function\"\n",
    "    if isinstance(obj_l, str):\n",
    "        obj_l = [obj_l]\n",
    "    if len(obj_l) == 0:\n",
    "        obj_l = Salesforce.model_d.keys()\n",
    "    for obj_s in obj_l:\n",
    "        print(f\"Deleting {obj_s} objects from Salesforce\")\n",
    "        object_ids = self.get_sf_object_ids(obj_s)\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir , f\"{obj_s}.csv\")\n",
    "        print(f\"In Salesforce.delete_sf_objects: Deleting {len(object_ids)} {obj_s} objects using {file_path_s}\")\n",
    "        with open(file_path_s, 'w') as f:\n",
    "            f.write('Id\\n')\n",
    "            for id in object_ids:\n",
    "                f.write(id+'\\n')\n",
    "                \n",
    "        # force execute_job to use the csv file we just created        \n",
    "        self.execute_job(obj_s, 'delete', use_ATMS_data=False)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def escape_quotes(text):\n",
    "    # Escape single quotes\n",
    "    # text = re.sub(r\"\\'\", r\"\\\\'\", text)\n",
    "    text = re.sub(r\"\\'\", r\"_\", text)\n",
    "    # Escape double quotes\n",
    "    text = re.sub(r'\\\"', r'_', text)\n",
    "    # text = re.sub(r',', r'*', text) ## shouldn't be necessary with tab delimiter\n",
    "    # text = re.sub(r'\\\"', r'\\\\\"', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    " #### modify so parent fields use correct shit \n",
    "#### maybe use a spreadsheet to make life easier \n",
    "# Name,Mother_Of_Child__r.contactId\n",
    "# CustomObject1,123456\n",
    "\n",
    "mem_s = \"[].{membershipId__c: membershipId, \\\n",
    "    memberSince__c: memberSince, \\\n",
    "    updateDate__c: updateDate}\"\n",
    "\n",
    "memTerm_s = \"[].membershipTerms[].{membershipTermId__c: membershipTermId,\\\n",
    "membershipKey__r_1_membershipId__c:membershipKey,\\\n",
    "effectiveDate__c:effectiveDate,\\\n",
    "expiryDate__c:expiryDate,\\\n",
    "membershipType__c:membershipType,\\\n",
    "upgradeFromTermKey__c:upgradeFromTermKey,\\\n",
    "giftMembership__c:giftMembership,\\\n",
    "refunded__c:refunded,\\\n",
    "saleDetailKey__c:saleDetailKey,\\\n",
    "itemKey__c:itemKey}\"\n",
    "\n",
    "memMembers_s = \"[].membershipTerms[].membershipMembers[].{membershipMemberId__c:membershipMemberId,\\\n",
    "membershipTermKey__r_1_membershipTermId__c:membershipTermKey,\\\n",
    "cardNumber__c:cardNumber,\\\n",
    "membershipNumber__c:membershipNumber,\\\n",
    "cardStatus__c:cardStatus,\\\n",
    "contactKey__r_1_contactId__c:contactKey,\\\n",
    "displayName__c:displayName}\"\n",
    "\n",
    "@patch\n",
    "def process_memberships(self: Salesforce ):\n",
    "    \"\"\"Unpack memberships data from atms object and write to membership, membership_terms, and membership_members csv files.\n",
    "    \n",
    "    We could modify this function to only process one of Memmbership, MembershipTerm, or MembershipMember.\n",
    "    \"\"\"\n",
    "    # print(\"Processing memberships data\")\n",
    "    # custom objects need '__c' suffix\n",
    "    mem_d = { 'memberships': {'fname':'Membership__c.csv', 'jmespath': mem_s},\n",
    "               'membership_terms': {'fname':'MembershipTerm__c.csv','jmespath': memTerm_s},\n",
    "               'membership_members': {'fname': 'MembershipMember__c.csv', 'jmespath': memMembers_s}\n",
    "                }\n",
    "            \n",
    "\n",
    "    if not ('memberships' in self.atms.obj_d):\n",
    "        self.atms.load_data_file_to_dict('memberships')\n",
    "        assert 'memberships' in self.atms.obj_d, f\"memberships not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    \n",
    "    atms_d = self.atms.obj_d['memberships']\n",
    "\n",
    "    for key, v_pair in mem_d.items():\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir, v_pair['fname'])\n",
    "        dict_l = jp.search(v_pair['jmespath'], atms_d)\n",
    "        # print(f\"Salesforce: Writing {len(dict_l)} {key} objects to {file_path_s}\")\n",
    "        with open(file_path_s, 'w') as f:\n",
    "            if len(dict_l) == 0:\n",
    "                # print(f\"Warning: no {key} objects found\")\n",
    "                continue\n",
    "            # hack to create header with a dot in it, jmespath won't do it\n",
    "            f.write('\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()]) + '\\n') # header\n",
    "            for d in dict_l:\n",
    "                #changed this to not write None for empty values, eg \"\" for null and false (a default value)\n",
    "                f.write('\\t'.join([str(v) if v else \"\" for v in d.values()]) + '\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_sales(self: Salesforce ):\n",
    "    search_s = \"[].{saleId__c : saleKey,\\\n",
    "            saleAmount__c : saleAmount,\\\n",
    "            paymentAmount__c : paymentAmount,\\\n",
    "            saleDate__c : saleDate,\\\n",
    "            active__c : active,\\\n",
    "            terminalKey__c : terminalKey,\\\n",
    "            booking_bookingId__c   : booking.bookingId,\\\n",
    "            booking_bookingContactKey__c : booking.bookingContactKey,\\\n",
    "            booking_contactKey__r_1_contactId__c : booking.contactKey,\\\n",
    "            booking_contactIndividualKey__c : booking.contactIndividualKey,\\\n",
    "            booking_contactOrganizationKey__c : booking.contactOrganizationKey,\\\n",
    "            booking_displayName__c : booking.displayName,\\\n",
    "            booking_firstName__c : booking.firstName,\\\n",
    "            booking_lastName__c : booking.lastName,\\\n",
    "            booking_email__c : booking.email,\\\n",
    "            booking_phone__c : booking.phone}\"\n",
    "            # eventDate__c : eventDate,\\\n",
    "            # booking_contactKey__c : booking_contactKey,\\\n",
    "    \n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Sale__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Sales' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        # hack to create header with a dot in it, jmespath won't do it\n",
    "        header = '\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()])\n",
    "        f.write(header + '\\n') # header\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_tickets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_tickets(self: Salesforce ):\n",
    "    search_s = \"[].tickets[].{ticketId__c : ticketKey,\\\n",
    "        saleKey__r_1_saleId__c : saleKey,\\\n",
    "        saleDetailKey__r_1_saleDetailId__c : saleDetailKey,\\\n",
    "        itemDescription__c : itemDescription,\\\n",
    "        ticketDisplay__c : ticketDisplay}\"\n",
    "        # scheduleDate__c : scheduleDate,\\\n",
    "        # scheduleEndDate__c : scheduleEndDate,\\\n",
    "\n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Ticket__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Ticket' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        # hack to create header with a dot in it, jmespath won't do it\n",
    "        header = '\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()])\n",
    "        f.write(header + '\\n') # header\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_saleDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_saleDetails(self: Salesforce ):\n",
    "    search_s = \"[].saleDetails[].{\\\n",
    "        saleDetailId__c : saleDetailId,\\\n",
    "        itemKey__c : itemKey,\\\n",
    "        scheduleKey__c : scheduleKey,\\\n",
    "        rateKey__c : rateKey,\\\n",
    "        categoryKey__c : categoryKey,\\\n",
    "        itemCategory__c : itemCategory,\\\n",
    "        pricingPriceKey__c : pricingPriceKey,\\\n",
    "        itemPrice__c : itemPrice,\\\n",
    "        itemTotal__c : itemTotal,\\\n",
    "        couponTotal__c : couponTotal,\\\n",
    "        discountTotal__c : discountTotal,\\\n",
    "        total__c : total,\\\n",
    "        revenueDate__c : revenueDate,\\\n",
    "        refundReason__c : refundReason,\\\n",
    "        refundReasonKey__c : refundReasonKey,\\\n",
    "        systemPriceOverride__c : systemPriceOverride,\\\n",
    "        membershipTermKey__r_1_membershipTermId__c : membershipTermKey,\\\n",
    "        saleKey__r_1_saleId__c : saleId}\" \n",
    "    \n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'SaleDetail__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'SaleDetail' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        # hack to create header with a dot in it, jmespath won't do it\n",
    "        header = '\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()])\n",
    "        f.write(header + '\\n') # header\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "search_s = \"[].{LastName: lastName,\\\n",
    "    FirstName: firstName,\\\n",
    "    MailingPostalCode: addresses[0].postalZipCode,\\\n",
    "    MailingCity: addresses[0].city,\\\n",
    "    MailingStreet: addresses[0].line1, \\\n",
    "    MailingCountry: addresses[0].country, \\\n",
    "    Phone: phones[?phoneType == 'Business'].phoneNumber | [0],\\\n",
    "    Email: emails[0].address[0],\\\n",
    "    contactId__c: contactId}\"\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_contacts(self: Salesforce ):\n",
    "    \"\"\" unpack contacts data from atms object and write to contacts csv file.\"\"\"\n",
    "    # print(\"process_contacts\")\n",
    "    if not ('contacts' in self.atms.obj_d):\n",
    "        self.atms.load_data_file_to_dict('contacts')\n",
    "        assert 'contacts' in self.atms.obj_d, f\"contacts not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    \n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Contact.csv')\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['contacts'])\n",
    "\n",
    "    # if contact record has no LastName then\n",
    "    for r in dict_l:\n",
    "        if r['LastName'] == None:\n",
    "            r['LastName'] = 'Not Provided'\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Contact' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        header = columnDelimiter.join(dict_l[0].keys())\n",
    "        f.write(header+'\\n')\n",
    "        for item in dict_l:\n",
    "            l = [escape_quotes(str(v)) if v else \" \" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def process_objects(self: Salesforce,\n",
    "                    sf_object_s: str = \"\",\n",
    "                    use_ATMS_data: bool = True\n",
    "                    ):\n",
    "\n",
    "    valid_obj_l = ['Contact', 'Membership__c', 'MembershipTerm__c', 'MembershipMember__c', 'Sale__c', 'Ticket__c','SaleDetail__c']\n",
    "    if sf_object_s not in valid_obj_l:\n",
    "        print(f\"sf_object_s must be one of {', '.join(valid_obj_l)}\")\n",
    "        return\n",
    "        \n",
    "    print(\"Salesforce.process_objects: sf_object_s :\",sf_object_s)\n",
    "\n",
    "    if sf_object_s == 'SaleDetail__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_saleDetails()\n",
    "\n",
    "    if sf_object_s == 'Ticket__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_tickets()\n",
    "\n",
    "    if sf_object_s == 'Sale__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_sales()\n",
    "    \n",
    "    if sf_object_s == 'Contact' and use_ATMS_data:\n",
    "        # this creates a file Contact.csv in the class_download_dir\n",
    "        self.process_contacts()\n",
    "    \n",
    "    if sf_object_s in ['Membership__c', 'MembershipMember__c', 'MembershipTerm__c'] and use_ATMS_data:\n",
    "        # this creates files Membership__c.csv, MembershipMember__c.csv, MembershipTerm__c.csv in the class_download_dir\n",
    "        self.process_memberships()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def execute_job(self: Salesforce, \n",
    "        sf_object_s : str = None, # Salesforce API object name\n",
    "        operation : str = None, # REST operation, e.g. insert, upsert, delete\n",
    "        max_trys : int = 20, # max number of times to try to get job status\n",
    "        external_id : str = 'External_Id__c', # name of the field that is the unique identifier to ATMS\n",
    "        use_ATMS_data : bool = True, # if True, update SF data directly from ATMS data, otherwise use previous data\n",
    "        # **kwargs : dict # additional parameters to pass to the REST API\n",
    "        ):\n",
    "    \"\"\"Test loading a Salesforce object with data from a local file\"\"\"\n",
    "    print(\"execute_job\")\n",
    "    \n",
    "\n",
    "    ## translate dictionaries to csv files\n",
    "    self.process_objects(sf_object_s=sf_object_s, use_ATMS_data=use_ATMS_data)\n",
    "\n",
    "    ## start data transfer to Salesforce server\n",
    "    self.create_job(sf_object=sf_object_s, operation=operation, external_id=external_id)\n",
    "    self.upload_csv(sf_object_s, use_download_dir_b = operation == 'delete')\n",
    "    self.close_job()\n",
    "\n",
    "    counter = 0\n",
    "    sleep_time = 3\n",
    "    \n",
    "    job_status = self.job_status()['state']\n",
    "    print(\"job status:\", self.job_status()['state'])\n",
    "\n",
    "    while job_status !='JobComplete' and job_status !='Failed' and counter < max_trys:\n",
    "        print(f\"waiting for job to complete, try {counter}, status: {self.job_status()['state']}\")\n",
    "        counter += 1\n",
    "        time.sleep(sleep_time)\n",
    "        job_status = self.job_status()['state']\n",
    "\n",
    "    print(\"Failed results:\")\n",
    "    print(self.failed_results().text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_fields from Salesforce for an object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_fields(self:Salesforce, \n",
    "               obj:str # the name of the Salesforce object\n",
    "               ) -> requests.Response:\n",
    "    \"\"\"Get the fields for a given Salesforce object\"\"\"\n",
    "    sf_headers = { 'Authorization': f\"Bearer {sf._sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/sobjects/{obj}/describe\"\n",
    "    # print(url)\n",
    "    response = requests.request(\"GET\", url, headers=sf_headers)\n",
    "    print(response)\n",
    "    r = response.json()\n",
    "    if response.status_code == 200:\n",
    "        r = response.json()\n",
    "        names = jp.search(\"fields[].name\",r)\n",
    "        return names\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `retrieve_atms_records_by_contactId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def retrieve_atms_records_by_contactId(\n",
    "    self: Salesforce, # the Salesforce object\n",
    "    contactId_l : list # list of contactIds to retrieve\n",
    "    ):\n",
    "    \"\"\"Retrieve ATMS records for a list of contactIds and write them to json files\"\"\"\n",
    "    for obj in ['sales', 'contacts', 'memberships']:\n",
    "        # does this not get written to file?, no. it does not. and we don't want it to because we're working on proccessing rn.\n",
    "        self.atms.fetch_data_by_contactIds(obj, contactId_l) \n",
    "\n",
    "    # write data to json files\n",
    "    self.atms.write_data_to_json_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `retrieve_atms_records_by_contactId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'atms_download' already exists.\n",
      "Directory 'atms_download' already exists.\n",
      "my id is 3pqh8wwd\n",
      "144164\n",
      "we have contact_id 144164 and obj is sales\n",
      "http://crm-api-telus.atmsplus.com/api/sales/contact/144164\n",
      "[{'saleKey': '310928', 'saleAmount': '123.0500', 'paymentAmount': '123.0500', 'saleDate': '2005-08-19T10:23:33.03', 'active': True, 'terminalKey': 4, 'ticketCount': 0, 'eventDate': None, 'booking': {'bookingId': 48112, 'bookingContactKey': 192717, 'bookingContactType': 'Primary     ', 'contactKey': 144164, 'contactIndividualKey': 137810, 'contactOrganizationKey': 5649, 'displayName': 'Pelton, Donna', 'firstName': 'Donna', 'lastName': 'Pelton', 'email': 'tdpelton@shaw.ca', 'phone': '7804592956'}, 'saleComment': None, 'saleDetails': [{'saleDetailId': 760379, 'saleId': 310928, 'itemKey': 7, 'scheduleKey': None, 'rateKey': 3, 'categoryKey': None, 'itemQuantity': 1, 'pricingPriceKey': 207, 'itemPrice': 105.0, 'itemTotal': 105.0, 'couponTotal': 0.0, 'discountTotal': 0.0, 'total': 123.05, 'revenueDate': '2005-08-19T00:00:00', 'refundReasonKey': None, 'systemPriceOverride': 'N', 'membershipTermKey': None, 'taxTotal': 8.05, 'surchargeTotal': 10.0, 'surchargeTaxTotal': 0.0, 'pendingShiftKey': 11879, 'pendingTerminalKey': 4, 'pendingDateTimeStamp': '2005-08-19T10:23:33.233', 'pendingUser': 'wedgej              ', 'finalStatus': 1, 'finalShiftKey': 11879, 'finalTerminalKey': 4, 'finalDateTimeStamp': '2005-08-19T10:24:00.67', 'finalUser': 'wedgej              ', 'refundQuantity': 0, 'firstScheduleDetailKey': None, 'itemDescription': 'Family Membership', 'rateDescription': 'Family (Old)', 'categoryDescription': None, 'scheduleDate': None}], 'tickets': []}]\n",
      "4708\n",
      "we have contact_id 4708 and obj is sales\n",
      "http://crm-api-telus.atmsplus.com/api/sales/contact/4708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[275], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sf \u001b[39m=\u001b[39m Salesforce()\n\u001b[1;32m      4\u001b[0m pelton_ids \u001b[39m=\u001b[39m [ \u001b[39m4708\u001b[39m, \u001b[39m119430\u001b[39m, \u001b[39m119431\u001b[39m, \u001b[39m144164\u001b[39m,\u001b[39m144165\u001b[39m, \u001b[39m144166\u001b[39m, \u001b[39m144167\u001b[39m, \u001b[39m415645\u001b[39m, \u001b[39m73012\u001b[39m, \u001b[39m105765\u001b[39m, \u001b[39m342765\u001b[39m ]\n\u001b[0;32m----> 5\u001b[0m sf\u001b[39m.\u001b[39;49mretrieve_atms_records_by_contactId(pelton_ids)\n\u001b[1;32m      7\u001b[0m \u001b[39m# number of objects in dictionary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(sf\u001b[39m.\u001b[39matms\u001b[39m.\u001b[39mobj_d) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m([\u001b[39m'\u001b[39m\u001b[39msales\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontacts\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmemberships\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwrong number of objects in dictionary \u001b[39m\u001b[39m{\u001b[39;00msf\u001b[39m.\u001b[39matms\u001b[39m.\u001b[39mobj_d\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[274], line 11\u001b[0m, in \u001b[0;36mretrieve_atms_records_by_contactId\u001b[0;34m(self, contactId_l)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Retrieve ATMS records for a list of contactIds and write them to json files\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39msales\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontacts\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmemberships\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[39m# does this not get written to file?, no. it does not. and we don't want it to because we're working on proccessing rn.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     sf\u001b[39m.\u001b[39;49matms\u001b[39m.\u001b[39;49mfetch_data_by_contactIds(obj, contactId_l) \n\u001b[1;32m     13\u001b[0m \u001b[39m# write data to json files\u001b[39;00m\n\u001b[1;32m     14\u001b[0m sf\u001b[39m.\u001b[39matms\u001b[39m.\u001b[39mwrite_data_to_json_files()\n",
      "File \u001b[0;32m~/Documents/Github/doubledot/doubledot/ATMS_api.py:323\u001b[0m, in \u001b[0;36mfetch_data_by_contactIds\u001b[0;34m(self, obj_s, id_l, store_data_b)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m contact_id \u001b[39min\u001b[39;00m id_s: \u001b[39m#pelton_df.contactKey.unique():\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39mprint\u001b[39m(contact_id)\n\u001b[0;32m--> 323\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_telus_data(obj_s, contact_id\u001b[39m=\u001b[39;49mcontact_id)\n\u001b[1;32m    324\u001b[0m     data_d \u001b[39m=\u001b[39m r[\u001b[39m'\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    325\u001b[0m     \u001b[39mprint\u001b[39m(data_d)\n",
      "File \u001b[0;32m~/Documents/Github/doubledot/doubledot/ATMS_api.py:283\u001b[0m, in \u001b[0;36mget_telus_data\u001b[0;34m(self, obj, offset, since_date, count, contact_id)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mprint\u001b[39m(vantix_data_url)    \n\u001b[1;32m    282\u001b[0m \u001b[39m# response = requests.request(\"GET\", vantix_data_url, headers=v_headers, data={}).json()\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, vantix_data_url, headers\u001b[39m=\u001b[39;49mv_headers, data\u001b[39m=\u001b[39;49m{})\n\u001b[1;32m    285\u001b[0m \u001b[39m# inform caller we're done if we get fewer records than requested\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# BUT there could still be an error in the response\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m: response, \u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m:  response\u001b[39m.\u001b[39mstatus_code\u001b[39m!=\u001b[39m\u001b[39m200\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(response\u001b[39m.\u001b[39mjson()) \u001b[39m<\u001b[39m count}\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    488\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    489\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    490\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    491\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    492\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    497\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vantix/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# 415645 73012 105765 and 342765 are also good contacts to bring in when you are rea and organization 133156\n",
    "sf = Salesforce()\n",
    "pelton_ids = [ 4708, 119430, 119431, 144164,144165, 144166, 144167, 415645, 73012, 105765, 342765 ]\n",
    "sf.retrieve_atms_records_by_contactId(pelton_ids)\n",
    "\n",
    "# number of objects in dictionary\n",
    "assert len(sf.atms.obj_d) == len(['sales', 'contacts', 'memberships','items']), f\"wrong number of objects in dictionary {sf.atms.obj_d.keys()}\"\n",
    "# number of files in directory\n",
    "assert len(os.listdir(sf.atms.download_dir)) == len(['sales', 'contacts', 'memberships','items']), f\"wrong number of files in directory {sf.atms.download_dir}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(file_path : str, idx: str = None):\n",
    "    df = pd.read_csv('sf_download/'+file_path, sep='\\t').drop_duplicates(subset=idx)\n",
    "    df.to_csv('sf_download/'+file_path+'2',sep='\\t', index=False)\n",
    "    return df\n",
    "\n",
    "def has_duplicates(file_s: str, idx: str = None):\n",
    "    df = pd.read_csv('sf_download/'+file_s, sep='\\t')\n",
    "    return (df.shape)[0] != (df.drop_duplicates( subset=idx).shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "## this will have only contacts that are members, and only members that are contacts. \n",
    "## is possible to have duplicates if a contact is a member of more than one membership?\n",
    "## yes but the duplicated fields will only be in one group or the other\n",
    "## so we separate them and then reduce them\n",
    "## but we should really just make a function for this...\n",
    "\n",
    "## given two dataframes and two fields return two dataframes that are subsets of the original dataframes for which the two fields match\n",
    "def match_df(df1, df2, field1, field2):\n",
    "    df1 = df1[df1[field1].isin(df2[field2])]\n",
    "    df2 = df2[df2[field2].isin(df1[field1])]\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# test that all external ids are unique\n",
    "# test that all lookups are valid\n",
    "\n",
    "def test_lookup_fields(df_d):\n",
    "    for fromKey in Salesforce.model_d.keys():\n",
    "        # verify that external id is unique\n",
    "        assert df_d[fromKey][Salesforce.model_d[fromKey]['external_id']].is_unique, f\"external id not unique for {fromKey}\"\n",
    "\n",
    "        # verify that all lookups are valid \n",
    "        r_cols = [col for col in df_d[fromKey].columns if re.search('__r\\.', col)]\n",
    "        for col in r_cols:\n",
    "            col_matches = re.search('(.*)__r\\.(.*)', col)\n",
    "            fromField = col\n",
    "            lookupField = col_matches.group(1)+'__c'\n",
    "            toField = col_matches.group(2)\n",
    "            parentTable = Salesforce.model_d[fromKey]['lookups_d'][lookupField]\n",
    "            fromColumn = df_d[fromKey][fromField]\n",
    "            toColumn = df_d[parentTable][toField]\n",
    "            assert (fromColumn.isin(toColumn)).all(), f\"bad lookup: {fromKey} {fromField} {toField}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti    \n",
    "## these funcs all use the global variable df_d\n",
    "\n",
    "# function that returns all the fields that point to a given foreign key\n",
    "def fields_pointing_to_foreign_key(\n",
    "        foreign_key : str, \n",
    "        df_d : dict\n",
    "        ) -> list:\n",
    "    return_list = []\n",
    "    for name, df in df_d.items():\n",
    "        for col in df.columns:\n",
    "            if re.search(foreign_key[:-1], col) and  len(col)>len(foreign_key)+3:\n",
    "                return_list.append((name, col))\n",
    "    return return_list\n",
    "\n",
    "# function that takes a foreign key and returns a set of all value that point to it\n",
    "def get_pointing_foreign_key_values(\n",
    "    foreign_key: str, \n",
    "    df_d : dict\n",
    "    ) -> set:\n",
    "    return_set = set()\n",
    "    table_cols_l = fields_pointing_to_foreign_key(foreign_key, df_d)\n",
    "    for table, col in table_cols_l:\n",
    "        return_set.update(set(df_d[table][col]))\n",
    "    return return_set\n",
    "\n",
    "\n",
    "def reduce_to_referenced_rows(\n",
    "    df_d : dict\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    given a dictionary of dataframes, return a dictionary of dataframes where only rows whose foreign key is referenced by another table are kept\n",
    "    \"\"\"\n",
    "    # df2_d should be a dictionary of perfect dataframes\n",
    "    ## only rows whose foreign key is referenced by another table should be kept\n",
    "    df2_d = {}\n",
    "    for k, v in Salesforce.model_d.items():\n",
    "        foreign_key = v['external_id']\n",
    "\n",
    "        # keep all rows if no other table points to this one\n",
    "        if len(fields_pointing_to_foreign_key(foreign_key, df_d)) == 0:\n",
    "            df2_d[k] = df_d[k]\n",
    "            print(k,'*', len(df_d[k]), len(df2_d[k]))\n",
    "            continue\n",
    "        point_to_foreign_key = get_pointing_foreign_key_values(foreign_key, df_d)\n",
    "        # print(k, s) \n",
    "        keep_b = df_d[k][foreign_key].isin(point_to_foreign_key)\n",
    "        df2_d[k] = df_d[k][keep_b]\n",
    "        print(k,len(df_d[k]),  len(df2_d[k]))\n",
    "    return df2_d\n",
    "\n",
    "# i want to write these to file and send them to salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_d = reduce_to_referenced_rows(df_d)\n",
    "# pd.DataFrame(_d)\n",
    "for k, v in _d.items():\n",
    "    display(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `perfect_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# starting from atms object dictionary, create a dictionary of dataframes for all SF objects\n",
    "# using this dictionary df_d, we can then remove duplicates of rows with same external_id\n",
    "# and remove any row which has a lookup to a non-existent foreign key\n",
    "\n",
    "@patch\n",
    "def perfect_data(self: Salesforce) -> dict:\n",
    "    assert len(self.atms.obj_d) == 4, 'atms dictionaries not available'\n",
    "    obj_l = Salesforce.model_d.keys()\n",
    "    for obj in obj_l:\n",
    "        # write atms dictionaries to csv file, if dictionary there - otherwise exception\n",
    "        self.process_objects(obj)\n",
    "\n",
    "    # create a dictionary of dataframes for all SF objects  \n",
    "    df_d = {}\n",
    "    for i in Salesforce.model_d.keys(): \n",
    "        df_d[i] = pd.read_csv('sf_download/'+i+'.csv', sep='\\t')\n",
    "    try:\n",
    "        print(\"this should fail\")\n",
    "        test_lookup_fields(df_d)    \n",
    "    except:\n",
    "        print(\"and it did. good.\")\n",
    "        \n",
    "    for i in Salesforce.model_d.keys(): \n",
    "        # remove duplicates of rows with same external_id\n",
    "        print(\"dropping duplicates for \", i, \" on \", self.model_d[i]['external_id'],\"...\")\n",
    "        df_d[i].drop_duplicates(subset= Salesforce.model_d[i]['external_id'], inplace=True)\n",
    "\n",
    "    # remove any row which has a lookup to a non-existent foreign key\n",
    "    for obj,relations in self.model_d.items():\n",
    "        print(obj)\n",
    "        for fromField, parent in relations['lookups_d'].items():\n",
    "            parentExternalId = Salesforce.model_d[parent]['external_id']\n",
    "            toColumn = df_d[parent][parentExternalId]\n",
    "\n",
    "            # combine from field and parent external id to get Salesforce lookup field\n",
    "            newFromField = fromField[:-1]+'r.'+parentExternalId\n",
    "            fromColumn = df_d[obj][newFromField]\n",
    "            indGood_b = fromColumn.isin(toColumn)\n",
    "            good_b = indGood_b.sum() == len(indGood_b)\n",
    "            if not good_b:\n",
    "                # print('bad lookup: ', obj, newFromField, parentExternalId,len(indGood_b), len(indGood_b) - indGood_b.sum())\n",
    "                df_d[obj]= match_df(df_d[obj], df_d[parent], newFromField, parentExternalId)[0]\n",
    "\n",
    "    try:\n",
    "        print(\"this should NOT fail\")\n",
    "        test_lookup_fields(df_d)\n",
    "    except:\n",
    "        print(\"but it did. Bad.\")\n",
    "        raise Exception(\"bad lookup\")\n",
    "    finally:\n",
    "        print(\"finally, it did not fail. Good.\")\n",
    "\n",
    "    df2_d = reduce_to_referenced_rows(df_d)\n",
    "    return df2_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `write_dict_to_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "@patch\n",
    "def write_dict_to_csv(\n",
    "    self: Salesforce,\n",
    "    df2_d : dict\n",
    "    ):\n",
    "    # write dictionary of dataframes to upload directory for all SF objects  \n",
    "    path = Salesforce.class_upload_dir\n",
    "    for k in Salesforce.model_d.keys(): \n",
    "        with open(path+k+'.csv', 'w') as f:\n",
    "            f.write(df2_d[k].to_csv(sep='\\t', index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `upload_csv_to_sf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def upload_csv_to_sf(\n",
    "    self : Salesforce,\n",
    "    clean_sf : list | bool = [],\n",
    "    clean_all : bool = False,\n",
    "    operation : str = 'upsert'\n",
    "    ):\n",
    "\n",
    "    # clean SF if desired\n",
    "    if clean_all:\n",
    "        self.delete_sf_objects()\n",
    "    else:\n",
    "        for obj in clean_sf:\n",
    "            self.delete_sf_objects(obj)\n",
    "\n",
    "    # upload all data to SF\n",
    "    for obj,relations in Salesforce.model_d.items():\n",
    "        print(obj, relations['external_id'] )\n",
    "        self.execute_job(obj, 'upsert', external_id=relations['external_id'], use_ATMS_data=False) \n",
    "        sleep(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{mermaid}\\ngraph TD;\\n    Membership-->MemberTerm;\\n    MemberTerm-->Member;\\n    Users-->Member;\\n    \\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"\"\"{mermaid}\n",
    "graph TD;\n",
    "    Membership-->MemberTerm;\n",
    "    MemberTerm-->Member;\n",
    "    Users-->Member;\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "graph TD;\n",
    "    Membership-->MemberTerm;\n",
    "    MemberTerm-->Member;\n",
    "    Users-->Member;\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vantix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
