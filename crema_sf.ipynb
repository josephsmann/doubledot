{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doubledot.Salesforce\n",
    "> Salesforce class for transfering data from Vantix to Salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp crema_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "import io\n",
    "from nbdev.showdoc import *\n",
    "import requests\n",
    "import json\n",
    "import jmespath as jp\n",
    "import re\n",
    "from time import sleep\n",
    "from fastcore.basics import patch\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "from doubledot.ATMS_api import ATMS_api as ATMS\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salesforce @property sf_access_token @staticmethod list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "## Module for Salesforce API\n",
    "\n",
    "class Salesforce:\n",
    "    \"\"\"Class for Salesforce API\"\"\"\n",
    "    class_download_dir = os.path.join(os.getcwd(),'sf_download')\n",
    "    class_upload_dir = os.path.join(os.getcwd(),'sf_upload')\n",
    "    transfer_lock = False # lock to prevent multiple transfers at once - not implemented yet. but probably necessary to work with withh nbdev_test \n",
    "\n",
    "    ## Salesforce table relationships \n",
    "    ## these are also orderd by dependency\n",
    "    model_d = {     'Contact'               :{ 'lookups_d': dict(), 'external_id':'contactId__c'},\n",
    "                    'Membership__c'         :{ 'lookups_d': dict(), 'external_id':'membershipId__c'},\n",
    "                    'MembershipTerm__c'     :{ 'lookups_d': {'membershipKey__c': 'Membership__c'}, 'external_id':'membershipTermId__c'},\n",
    "                    'MembershipMember__c'   :{ 'lookups_d': {'contactKey__c': 'Contact', 'membershipTermKey__c': 'MembershipTerm__c' }, 'external_id':'membershipMemberId__c'},\n",
    "                    # in SF change saleId__c to saleId__c\n",
    "                    'Sale__c'               :{ 'lookups_d': {'booking_contactKey__c': 'Contact'}, 'external_id':'saleId__c'},\n",
    "                    # in SF change membershipTermKey__c to membershipTermKey__c\n",
    "                    'SaleDetail__c'         :{ 'lookups_d': {'membershipTermKey__c':'MembershipTerm__c', 'saleKey__c': 'Sale__c'}, 'external_id':'saleDetailId__c'},\n",
    "                    # 'SaleDetail__c'         :{ 'lookups_d': {'membershipTermKey__c':'MembershipTerm__c', 'saleId__c': 'Sale__c'}, 'external_id':'saleDetailId__c'},\n",
    "                    # in SF change tickeKey__c to ticketId__c\n",
    "                    # change saleId__c to saleKey__c in Ticket__c\n",
    "                    'Ticket__c'             :{ 'lookups_d': {'saleKey__c': 'Sale__c', 'saleDetailKey__c': 'SaleDetail__c'}, 'external_id':'ticketId__c'} \n",
    "            }\n",
    "\n",
    "    def __init__(self):\n",
    "        # set up access token \n",
    "        self._sf_access_token = self.get_token_with_REST()\n",
    "        self.bulk_job_id = None\n",
    "        self._atms = None\n",
    "\n",
    "        # create unique download directory per instance\n",
    "        if not os.path.exists(Salesforce.class_download_dir):\n",
    "            os.makedirs(Salesforce.class_download_dir)\n",
    "            print(f\"Directory 'atms_download' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory 'atms_download' already exists.\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sf_access_token(\n",
    "        self \n",
    "     ) -> str : #the access toke\n",
    "        \"\"\"a @property\n",
    "        retrieve token for Salesforce - verifies that token is still valid and attempts to get a new one from Salesforce site if not\n",
    "        \"\"\"\n",
    "        if not(self.test_token()):\n",
    "            self._sf_access_token = self.get_token_with_REST()\n",
    "            # check to see if getting token worked\n",
    "            assert (self.sf_access_token), \"Fetching new token didn't fix problem\"\n",
    "        return self._sf_access_token\n",
    "\n",
    "\n",
    "    @property\n",
    "    def atms( self):\n",
    "        if not self._atms:\n",
    "            self._atms = ATMS()\n",
    "        return self._atms\n",
    "    \n",
    "    @staticmethod\n",
    "    def list_files():\n",
    "        return os.listdir(Salesforce.class_download_dir)\n",
    "\n",
    "show_doc(Salesforce.sf_access_token)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_token_with_REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_with_REST(self: Salesforce):\n",
    "    \"\"\"retieve the access token from Salesforce\n",
    "\n",
    "    Returns:\n",
    "        string: the access token \n",
    "    \"\"\"\n",
    "    with open('secrets.json') as f:\n",
    "        secrets = json.load(f)\n",
    "    \n",
    "    DOMAIN = secrets['instance']\n",
    "    payload = {\n",
    "        'grant_type': 'password',\n",
    "        'client_id': secrets['client_id'],\n",
    "        'client_secret': secrets['client_secret'],\n",
    "        'username': secrets['username'],\n",
    "        'password': secrets['password'] + secrets['security_token']\n",
    "    }\n",
    "    oauth_url = f'{DOMAIN}/services/oauth2/token'\n",
    "\n",
    "    auth_response = requests.post(oauth_url, data=payload)\n",
    "    return auth_response.json().get('access_token') ######## <<<<<<<<<<<<<<<< .       \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test_token(self: Salesforce):\n",
    "    \"\"\"Verify that token is still valid. If it isn't, it attempts to get a new one.\n",
    "\n",
    "    Returns:\n",
    "        boolean: true if token is valid, false otherwise\n",
    "    \"\"\"\n",
    "    sf_headers = { 'Authorization': f\"Bearer {self._sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    end_point =\"https://cremaconsulting-dev-ed.develop.my.salesforce.com\"\n",
    "    service = \"/services/data/v57.0/\"\n",
    "    r = requests.request(\"GET\", end_point+service+f\"limits\", headers=sf_headers, data={})\n",
    "    valid_token = r.status_code == 200\n",
    "    if not(valid_token): print(r.status_code, type(r.status_code))\n",
    "    return valid_token\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def create_job(self: Salesforce, \n",
    "                sf_object: str ='Contact', # the Salesforce object were going to operate on. \n",
    "                operation: str ='insert', # the database operation to use. Can be \"insert\",\"upsert\" or \"delete\"\n",
    "                external_id: str = 'External_Id__c' # when using \"upsert\", this field is used to identify the record\n",
    "                )-> requests.Response :\n",
    "    \"\"\"Get job_id from Salesforce Bulk API\n",
    "\n",
    "    \"\"\"\n",
    "    # Args: \n",
    "    #     sf_object (str, optional): the Salesforce object were going to operate on. Defaults to 'Contact'.\n",
    "    #     operation (str, optional): âˆ†. Defaults to 'insert'.\n",
    "    #     external_id (str, optional): the external id field for upsert operations. Defaults to 'External_Id__c'.\n",
    "    #     sf_object (str, optional): the Salesforce object were going to operate on. Defaults to 'Contact'.\n",
    "    #     operation (str, optional): the operation that will be used against the object. Defaults to 'insert'.\n",
    "    #     external_id (str, optional): the external id field for upsert operations. Defaults to 'External_Id__c'.\n",
    "    #     contentType (str, optional): the content type of the file. Defaults to 'CSV', 'JSON' also accepted.\n",
    "    # Returns: \n",
    "    #     response: a response object containg the job_id. For more information on the response object see https://www.w3schools.com/python/ref_requests_response.asp\n",
    "    #     a response object see https://www.w3schools.com/python/ref_requests_response.asp\n",
    "        \n",
    "    # Salesforce API docs: https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/create_job.htm    \n",
    "    url = \"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest\"\n",
    "\n",
    "    # https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/datafiles_prepare_csv.htm\n",
    "    ## we can set columnDelimiter to `,^,|,;,<tab>, and the default <comma>\n",
    "    # sets the object to Contact, the content type to CSV, and the operation to insert\n",
    "    payload_d = {\n",
    "        \"object\": sf_object,\n",
    "        \"contentType\": \"CSV\",\n",
    "        # set columnDelimiter to TAB instead of comma for ease of dealing with commas in address fields\n",
    "        #https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/create_job.htm\n",
    "        \"columnDelimiter\": \"TAB\", \n",
    "        \"operation\": operation\n",
    "    }\n",
    "    \n",
    "    # as per https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/walkthrough_upsert.htm\n",
    "    if operation=='upsert':\n",
    "        payload_d['externalIdFieldName']=external_id\n",
    "    payload = json.dumps(payload_d)\n",
    "    \n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    # print(response.text)\n",
    "    try:\n",
    "        self.bulk_job_id = response.json()['id']\n",
    "    except TypeError:\n",
    "        print(\"Bad response in Salesforce.create_job :\\n\", response.text)\n",
    "        \n",
    "    print(f\"Created job {self.bulk_job_id} for {sf_object} with operation {operation}\") \n",
    "    return response \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def upload_csv(self : Salesforce, \n",
    "                obj_s: str = \"\", # Salesforce object to upload \n",
    "                use_download_dir_b : bool = False, # \n",
    "                # num_rows: int = 100, # the number of rows to upload \n",
    "                ) -> requests.Response:\n",
    "    \"\"\"Using the job_id from the previous step, upload the csv file to the job\n",
    "\n",
    "    Args:\n",
    "        file (filepointer): file pointer to the csv filek\n",
    "    \"\"\"\n",
    "    # if not(file):\n",
    "    #     # throw error\n",
    "    #     assert False, \"File not found\"\n",
    "\n",
    "\n",
    "    assert obj_s in ['Contact', 'Membership__c', 'MembershipTerm__c', 'MembershipMember__c', 'Sale__c', 'Ticket__c', 'SaleDetail__c']\n",
    "\n",
    "    print(f\"Uploading job {self.bulk_job_id} of object {obj_s}\")\n",
    "\n",
    "    if use_download_dir_b:\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir , f\"{obj_s}.csv\")\n",
    "    else:\n",
    "        file_path_s = os.path.join(Salesforce.class_upload_dir , f\"{obj_s}.csv\")\n",
    "\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/batches\"\n",
    "\n",
    "    # replace all occurrences of '\\2019' with \\'\n",
    "    # we may have done this in ATMS already, but just in case\n",
    "    try:\n",
    "        for line in fileinput.input(files=file_path_s, inplace=True):\n",
    "            line = line.replace('\\u2019', \"'\")\n",
    "            print(line, end='') # this prints to the file instead of stdout (!!)\n",
    "\n",
    "        with open(file_path_s,'r') as payload:\n",
    "            headers = {\n",
    "                'Content-Type': 'text/csv',\n",
    "                'Authorization': f'Bearer {self.sf_access_token}'\n",
    "                }\n",
    "            response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found error in Saleforce.upload_csv: \", file_path_s)\n",
    "        return None\n",
    "    \n",
    "    return response\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## close_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def close_job(self: Salesforce):\n",
    "    # close the job (from Postman)\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"state\": \"UploadComplete\"\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"PATCH\", url, headers=headers, data=payload)\n",
    "\n",
    "    # print(response.text)\n",
    "    return response.json()\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## job_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export       \n",
    "# get job status (from Postman)\n",
    "@patch\n",
    "def job_status(self: Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "    'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## successful_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def successful_results(self : Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/successfulResults\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def failed_results(self: Salesforce):\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/jobs/ingest/{self.bulk_job_id}/failedResults\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {self.sf_access_token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    try:\n",
    "        print(pd.DataFrame(response.json()))\n",
    "    except:\n",
    "        print(response.text)\n",
    "    # \n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_sf_object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_sf_object_ids(self: Salesforce, \n",
    "                      object: str = 'Contact' # REST endpoint for data object\n",
    "                      ):\n",
    "    \"\"\"Get Safesforce IDs for a the specified object\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Retrieving Object Ids for {object} from Salesforce\")\n",
    "    sf_headers = { 'Authorization': f\"Bearer {self.sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    end_point =\"https://cremaconsulting-dev-ed.develop.my.salesforce.com\"\n",
    "    service = \"/services/data/v57.0/\"\n",
    "    r = requests.request(\"GET\", end_point+service+f\"query/?q=SELECT+Id+FROM+{object}\", headers=sf_headers, data={})\n",
    "    assert isinstance(r.json(), dict), f\"response: {r.json()}, header: {sf_headers}\"\n",
    "    object_ids = [d.get('Id') for d in r.json()['records']]\n",
    "    while r.json()['done'] == False:\n",
    "        new_url = end_point+r.json()['nextRecordsUrl']\n",
    "        print(new_url)\n",
    "        r = requests.request(\"GET\", new_url, headers=sf_headers, data={})\n",
    "        print((r.json()))\n",
    "        fresh_object_ids = [d.get('Id') for d in r.json()['records']]\n",
    "        print(len(fresh_object_ids))   \n",
    "        object_ids+=fresh_object_ids\n",
    "        \n",
    "    print('total number of objects = ',len(object_ids))\n",
    "    return object_ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_sf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def delete_sf_objects(self: Salesforce, \n",
    "                      obj_l: str | list = []\n",
    "                      ):\n",
    "    \"\"\"Delete Salesforce objects from the specified list or string. Empty list deletes all objects.\"\"\"\n",
    "    # assert False, \"want to catch test calls to this function\"\n",
    "    if isinstance(obj_l, str):\n",
    "        obj_l = [obj_l]\n",
    "    if len(obj_l) == 0:\n",
    "        obj_l = Salesforce.model_d.keys()\n",
    "    for obj_s in obj_l:\n",
    "        print(f\"Deleting {obj_s} objects from Salesforce\")\n",
    "        object_ids = self.get_sf_object_ids(obj_s)\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir , f\"{obj_s}.csv\")\n",
    "        print(f\"In Salesforce.delete_sf_objects: Deleting {len(object_ids)} {obj_s} objects using {file_path_s}\")\n",
    "        with open(file_path_s, 'w') as f:\n",
    "            f.write('Id\\n')\n",
    "            for id in object_ids:\n",
    "                f.write(id+'\\n')\n",
    "                \n",
    "        # force execute_job to use the csv file we just created        \n",
    "        self.execute_job(obj_s, 'delete', use_ATMS_data=False)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def escape_quotes(text):\n",
    "    # Escape single quotes\n",
    "    # text = re.sub(r\"\\'\", r\"\\\\'\", text)\n",
    "    text = re.sub(r\"\\'\", r\"_\", text)\n",
    "    # Escape double quotes\n",
    "    text = re.sub(r'\\\"', r'_', text)\n",
    "    # text = re.sub(r',', r'*', text) ## shouldn't be necessary with tab delimiter\n",
    "    # text = re.sub(r'\\\"', r'\\\\\"', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_jmespath_to_csv(dict_l, file_path_s, keys):\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        header = '\\t'.join(keys)\n",
    "        f.write(header + '\\n') # header\n",
    "        if len(dict_l) == 0:\n",
    "            # print(f\"Warning: no {key} objects found\")\n",
    "            return\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    " #### modify so parent fields use correct shit \n",
    "#### maybe use a spreadsheet to make life easier \n",
    "# Name,Mother_Of_Child__r.contactId\n",
    "# CustomObject1,123456\n",
    "\n",
    "mem_keys = [\"membershipId__c\", \"memberSince__c\", \"updateDate__c\"]\n",
    "\n",
    "mem_s = \"[].{membershipId__c: membershipId, \\\n",
    "    memberSince__c: memberSince, \\\n",
    "    updateDate__c: updateDate}\"\n",
    "\n",
    "\n",
    "memTerm_keys = [\"membershipTermId__c\" ,\n",
    "\"membershipKey__r.membershipId__c\" ,\n",
    "\"effectiveDate__c\" ,\n",
    "\"expiryDate__c\" ,\n",
    "\"membershipType__c\" ,\n",
    "\"upgradeFromTermKey__c\" ,\n",
    "\"giftMembership__c\" ,\n",
    "\"refunded__c\" ,\n",
    "\"saleDetailKey__c\" ,\n",
    "\"itemKey__c\"] \n",
    "\n",
    "memTerm_s = \"[].membershipTerms[].{membershipTermId__c: membershipTermId,\\\n",
    "membershipKey__r_1_membershipId__c:membershipKey,\\\n",
    "effectiveDate__c:effectiveDate,\\\n",
    "expiryDate__c:expiryDate,\\\n",
    "membershipType__c:membershipType,\\\n",
    "upgradeFromTermKey__c:upgradeFromTermKey,\\\n",
    "giftMembership__c:giftMembership,\\\n",
    "refunded__c:refunded,\\\n",
    "saleDetailKey__c:saleDetailKey,\\\n",
    "itemKey__c:itemKey}\"\n",
    "\n",
    "memMembers_keys =  [ \"membershipMemberId__c\", \n",
    "\"membershipTermKey__r.membershipTermId__c\", \n",
    "\"cardNumber__c\", \n",
    "\"membershipNumber__c\", \n",
    "\"cardStatus__c\", \n",
    "\"contactKey__r.contactId__c\", \n",
    "\"displayName__c\" ]\n",
    "\n",
    "memMembers_s = \"[].membershipTerms[].membershipMembers[].{membershipMemberId__c:membershipMemberId,\\\n",
    "membershipTermKey__r_1_membershipTermId__c:membershipTermKey,\\\n",
    "cardNumber__c:cardNumber,\\\n",
    "membershipNumber__c:membershipNumber,\\\n",
    "cardStatus__c:cardStatus,\\\n",
    "contactKey__r_1_contactId__c:contactKey,\\\n",
    "displayName__c:displayName}\"\n",
    "\n",
    "@patch\n",
    "def process_memberships(self: Salesforce ):\n",
    "    \"\"\"Unpack memberships data from atms object and write to membership, membership_terms, and membership_members csv files.\n",
    "    \n",
    "    We could modify this function to only process one of Memmbership, MembershipTerm, or MembershipMember.\n",
    "    \"\"\"\n",
    "    # print(\"Processing memberships data\")\n",
    "    # custom objects need '__c' suffix\n",
    "    mem_d = { 'memberships': {'fname':'Membership__c.csv', 'jmespath': mem_s, 'keys': mem_keys},\n",
    "               'membership_terms': {'fname':'MembershipTerm__c.csv','jmespath': memTerm_s, 'keys': memTerm_keys},\n",
    "               'membership_members': {'fname': 'MembershipMember__c.csv', 'jmespath': memMembers_s, 'keys': memMembers_keys}\n",
    "                }\n",
    "            \n",
    "\n",
    "    if not ('memberships' in self.atms.obj_d):\n",
    "        self.atms.load_data_file_to_dict('memberships')\n",
    "        assert 'memberships' in self.atms.obj_d, f\"memberships not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    \n",
    "    atms_d = self.atms.obj_d['memberships']\n",
    "\n",
    "    for key, v_pair in mem_d.items():\n",
    "        file_path_s = os.path.join(Salesforce.class_download_dir, v_pair['fname'])\n",
    "        dict_l = jp.search(v_pair['jmespath'], atms_d)\n",
    "        # print(f\"Salesforce: Writing {len(dict_l)} {key} objects to {file_path_s}\")\n",
    "        write_jmespath_to_csv(dict_l, file_path_s, v_pair['keys'])\n",
    "        # with open(file_path_s, 'w') as f:\n",
    "        #     if len(dict_l) == 0:\n",
    "        #         # print(f\"Warning: no {key} objects found\")\n",
    "        #         continue\n",
    "        #     # hack to create header with a dot in it, jmespath won't do it\n",
    "        #     f.write('\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()]) + '\\n') # header\n",
    "        #     for d in dict_l:\n",
    "        #         #changed this to not write None for empty values, eg \"\" for null and false (a default value)\n",
    "        #         f.write('\\t'.join([str(v) if v else \"\" for v in d.values()]) + '\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_sales(self: Salesforce ):\n",
    "    keys = ['saleId__c', 'saleAmount__c', 'paymentAmount__c','saleDate__c', 'active__c', 'terminalKey__c',\n",
    "             'booking_bookingId__c', 'booking_bookingContactKey__c', 'booking_contactKey__r.contactId__c',\n",
    "               'booking_contactIndividualKey__c', 'booking_contactOrganizationKey__c', 'booking_displayName__c',\n",
    "                 'booking_firstName__c', 'booking_lastName__c', 'booking_email__c', 'booking_phone__c']\n",
    "    \n",
    "    search_s = \"[].{saleId__c : saleKey,\\\n",
    "            saleAmount__c : saleAmount,\\\n",
    "            paymentAmount__c : paymentAmount,\\\n",
    "            saleDate__c : saleDate,\\\n",
    "            active__c : active,\\\n",
    "            terminalKey__c : terminalKey,\\\n",
    "            booking_bookingId__c   : booking.bookingId,\\\n",
    "            booking_bookingContactKey__c : booking.bookingContactKey,\\\n",
    "            booking_contactKey__r_1_contactId__c : booking.contactKey,\\\n",
    "            booking_contactIndividualKey__c : booking.contactIndividualKey,\\\n",
    "            booking_contactOrganizationKey__c : booking.contactOrganizationKey,\\\n",
    "            booking_displayName__c : booking.displayName,\\\n",
    "            booking_firstName__c : booking.firstName,\\\n",
    "            booking_lastName__c : booking.lastName,\\\n",
    "            booking_email__c : booking.email,\\\n",
    "            booking_phone__c : booking.phone}\"\n",
    "            # eventDate__c : eventDate,\\\n",
    "            # booking_contactKey__c : booking_contactKey,\\\n",
    "    \n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Sale__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Sales' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        if len(dict_l) == 0:\n",
    "            # print(f\"Warning: no {key} objects found\")\n",
    "            return\n",
    "        # hack to create header with a dot in it, jmespath won't do it\n",
    "        header = '\\t'.join([s.replace('_1_','.') for s in dict_l[0].keys()])\n",
    "        # header = columnDelimiter.join(keys)\n",
    "        f.write(header + '\\n') # header\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_tickets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_tickets(self: Salesforce ):\n",
    "    keys = ['ticketId__c', 'saleKey__r.saleId__c', 'saleDetailKey__r.saleDetailId__c', 'itemDescription__c', 'ticketDisplay__c']\n",
    "    search_s = \"[].tickets[].{ticketId__c : ticketKey,\\\n",
    "        saleKey__r_1_saleId__c : saleKey,\\\n",
    "        saleDetailKey__r_1_saleDetailId__c : saleDetailKey,\\\n",
    "        itemDescription__c : itemDescription,\\\n",
    "        ticketDisplay__c : ticketDisplay}\"\n",
    "        # scheduleDate__c : scheduleDate,\\\n",
    "        # scheduleEndDate__c : scheduleEndDate,\\\n",
    "\n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Ticket__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Ticket' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        header = '\\t'.join(keys)\n",
    "        f.write(header + '\\n') # header\n",
    "        if len(dict_l) == 0:\n",
    "            # print(f\"Warning: no {key} objects found\")\n",
    "            return\n",
    "        for item in dict_l:\n",
    "            # changed this from single space to empty string if null\n",
    "            l = [escape_quotes(str(v)) if v else \"\" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_saleDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_saleDetails(self: Salesforce ):\n",
    "    search_s = \"[].saleDetails[].{\\\n",
    "        saleDetailId__c : saleDetailId,\\\n",
    "        itemKey__c : itemKey,\\\n",
    "        scheduleKey__c : scheduleKey,\\\n",
    "        rateKey__c : rateKey,\\\n",
    "        categoryKey__c : categoryKey,\\\n",
    "        itemCategory__c : itemCategory,\\\n",
    "        pricingPriceKey__c : pricingPriceKey,\\\n",
    "        itemPrice__c : itemPrice,\\\n",
    "        itemTotal__c : itemTotal,\\\n",
    "        couponTotal__c : couponTotal,\\\n",
    "        discountTotal__c : discountTotal,\\\n",
    "        total__c : total,\\\n",
    "        revenueDate__c : revenueDate,\\\n",
    "        refundReason__c : refundReason,\\\n",
    "        refundReasonKey__c : refundReasonKey,\\\n",
    "        systemPriceOverride__c : systemPriceOverride,\\\n",
    "        membershipTermKey__r_1_membershipTermId__c : membershipTermKey,\\\n",
    "        saleKey__r_1_saleId__c : saleId}\" \n",
    "    \n",
    "    keys = ['saleDetailId__c', 'itemKey__c', 'scheduleKey__c', 'rateKey__c', 'categoryKey__c', 'itemCategory__c', \n",
    "            'pricingPriceKey__c', 'itemPrice__c', 'itemTotal__c', 'couponTotal__c', 'discountTotal__c', 'total__c', \n",
    "            'revenueDate__c', 'refundReason__c', 'refundReasonKey__c', 'systemPriceOverride__c', \n",
    "            'membershipTermKey__r.membershipTermId__c', 'saleKey__r.saleId__c']\n",
    "\n",
    "    assert 'sales' in self.atms.obj_d, f\"sales not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['sales'])\n",
    "\n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'SaleDetail__c.csv')\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'SaleDetail' objects to {file_path_s}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "search_s = \"[].{LastName: lastName,\\\n",
    "    FirstName: firstName,\\\n",
    "    MailingPostalCode: addresses[0].postalZipCode,\\\n",
    "    MailingCity: addresses[0].city,\\\n",
    "    MailingStreet: addresses[0].line1, \\\n",
    "    MailingCountry: addresses[0].country, \\\n",
    "    Phone: phones[?phoneType == 'Business'].phoneNumber | [0],\\\n",
    "    Email: emails[0].address[0],\\\n",
    "    contactId__c: contactId}\"\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_contacts(self: Salesforce ):\n",
    "    \"\"\" unpack contacts data from atms object and write to contacts csv file.\"\"\"\n",
    "    # print(\"process_contacts\")\n",
    "    if not ('contacts' in self.atms.obj_d):\n",
    "        self.atms.load_data_file_to_dict('contacts')\n",
    "        assert 'contacts' in self.atms.obj_d, f\"contacts not in atms.obj_d {self.atms.obj_d.keys()}\"\n",
    "    \n",
    "    file_path_s = os.path.join(Salesforce.class_download_dir, 'Contact.csv')\n",
    "    dict_l = jp.search(search_s, self.atms.obj_d['contacts'])\n",
    "\n",
    "    # if contact record has no LastName then\n",
    "    for r in dict_l:\n",
    "        if r['LastName'] == None:\n",
    "            r['LastName'] = 'Not Provided'\n",
    "\n",
    "    # print(f\"Salesforce: Writing {len(dict_l)} 'Contact' objects to {file_path_s}\")\n",
    "    columnDelimiter = '\\t'\n",
    "    with open(file_path_s, 'w') as f:\n",
    "        if len(dict_l) == 0:\n",
    "            # print(f\"Warning: no {key} objects found\")\n",
    "            return\n",
    "        header = columnDelimiter.join(dict_l[0].keys())\n",
    "        f.write(header+'\\n')\n",
    "        for item in dict_l:\n",
    "            l = [escape_quotes(str(v)) if v else \" \" for v in item.values()]\n",
    "            f.write(columnDelimiter.join(l)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def process_objects(self: Salesforce,\n",
    "                    sf_object_s: str = \"\",\n",
    "                    use_ATMS_data: bool = True\n",
    "                    ):\n",
    "\n",
    "    valid_obj_l = ['Contact', 'Membership__c', 'MembershipTerm__c', 'MembershipMember__c', 'Sale__c', 'Ticket__c','SaleDetail__c']\n",
    "    if sf_object_s not in valid_obj_l:\n",
    "        print(f\"sf_object_s must be one of {', '.join(valid_obj_l)}\")\n",
    "        return\n",
    "        \n",
    "    print(\"Salesforce.process_objects: sf_object_s :\",sf_object_s)\n",
    "\n",
    "    if sf_object_s == 'SaleDetail__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_saleDetails()\n",
    "\n",
    "    if sf_object_s == 'Ticket__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_tickets()\n",
    "\n",
    "    if sf_object_s == 'Sale__c' and use_ATMS_data:\n",
    "        # this creates a file Sales__c.csv in the class_download_dir\n",
    "        self.process_sales()\n",
    "    \n",
    "    if sf_object_s == 'Contact' and use_ATMS_data:\n",
    "        # this creates a file Contact.csv in the class_download_dir\n",
    "        self.process_contacts()\n",
    "    \n",
    "    if sf_object_s in ['Membership__c', 'MembershipMember__c', 'MembershipTerm__c'] and use_ATMS_data:\n",
    "        # this creates files Membership__c.csv, MembershipMember__c.csv, MembershipTerm__c.csv in the class_download_dir\n",
    "        self.process_memberships()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def execute_job(self: Salesforce, \n",
    "        sf_object_s : str = None, # Salesforce API object name\n",
    "        operation : str = None, # REST operation, e.g. insert, upsert, delete\n",
    "        max_trys : int = 20, # max number of times to try to get job status\n",
    "        external_id : str = 'External_Id__c', # name of the field that is the unique identifier to ATMS\n",
    "        use_ATMS_data : bool = True, # if True, update SF data directly from ATMS data, otherwise use previous data\n",
    "        # **kwargs : dict # additional parameters to pass to the REST API\n",
    "        ):\n",
    "    \"\"\"Test loading a Salesforce object with data from a local file\"\"\"\n",
    "    print(\"execute_job\")\n",
    "    \n",
    "\n",
    "    ## translate dictionaries to csv files\n",
    "    self.process_objects(sf_object_s=sf_object_s, use_ATMS_data=use_ATMS_data)\n",
    "\n",
    "    ## start data transfer to Salesforce server\n",
    "    self.create_job(sf_object=sf_object_s, operation=operation, external_id=external_id)\n",
    "    self.upload_csv(sf_object_s, use_download_dir_b = operation == 'delete')\n",
    "    self.close_job()\n",
    "\n",
    "    counter = 0\n",
    "    sleep_time = 3\n",
    "    \n",
    "    job_status = self.job_status()['state']\n",
    "    print(\"job status:\", self.job_status()['state'])\n",
    "\n",
    "    while job_status !='JobComplete' and job_status !='Failed' and counter < max_trys:\n",
    "        print(f\"waiting for job to complete, try {counter}, status: {self.job_status()['state']}\")\n",
    "        counter += 1\n",
    "        time.sleep(sleep_time)\n",
    "        job_status = self.job_status()['state']\n",
    "\n",
    "    print(\"Failed results:\")\n",
    "    print(self.failed_results().text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_fields from Salesforce for an object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_fields(self:Salesforce, \n",
    "               obj:str # the name of the Salesforce object\n",
    "               ) -> requests.Response:\n",
    "    \"\"\"Get the fields for a given Salesforce object\"\"\"\n",
    "    sf_headers = { 'Authorization': f\"Bearer {self._sf_access_token}\", 'Content-Type': 'application/json' }\n",
    "    url = f\"https://cremaconsulting-dev-ed.develop.my.salesforce.com/services/data/v57.0/sobjects/{obj}/describe\"\n",
    "    # print(url)\n",
    "    response = requests.request(\"GET\", url, headers=sf_headers)\n",
    "    print(response)\n",
    "    r = response.json()\n",
    "    if response.status_code == 200:\n",
    "        r = response.json()\n",
    "        names = jp.search(\"fields[].name\",r)\n",
    "        return names\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `retrieve_atms_records_by_contactId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def retrieve_atms_records_by_contactId(\n",
    "    self: Salesforce, # the Salesforce object\n",
    "    contactId_l : list # list of contactIds to retrieve\n",
    "    ):\n",
    "    \"\"\"Retrieve ATMS records for a list of contactIds and write them to json files\"\"\"\n",
    "    for obj in ['sales', 'contacts', 'memberships']:\n",
    "        # does this not get written to file?, no. it does not. and we don't want it to because we're working on proccessing rn.\n",
    "        try:\n",
    "            self.atms.fetch_data_by_contactIds(obj, contactId_l) \n",
    "        except Exception as e :\n",
    "            print(f\"Error: with {obj} and {contactId_l}\")\n",
    "            raise e\n",
    "\n",
    "    # write data to json files\n",
    "    self.atms.write_data_to_json_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `retrieve_atms_records_by_contactId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# 415645 73012 105765 and 342765 are also good contacts to bring in when you are rea and organization 133156\n",
    "sf = Salesforce()\n",
    "pelton_ids = [ 4708, 119430, 119431, 144164,144165, 144166, 144167, 415645, 73012, 105765, 342765 ]\n",
    "sf.retrieve_atms_records_by_contactId(pelton_ids)\n",
    "\n",
    "# number of objects in dictionary\n",
    "assert len(sf.atms.obj_d) == len(['sales', 'contacts', 'memberships','items']), f\"wrong number of objects in dictionary {sf.atms.obj_d.keys()}\"\n",
    "# number of files in directory\n",
    "assert len(os.listdir(sf.atms.download_dir)) == len(['sales', 'contacts', 'memberships','items']), f\"wrong number of files in directory {sf.atms.download_dir}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(file_path : str, idx: str = None):\n",
    "    df = pd.read_csv('sf_download/'+file_path, sep='\\t').drop_duplicates(subset=idx)\n",
    "    df.to_csv('sf_download/'+file_path+'2',sep='\\t', index=False)\n",
    "    return df\n",
    "\n",
    "def has_duplicates(file_s: str, idx: str = None):\n",
    "    df = pd.read_csv('sf_download/'+file_s, sep='\\t')\n",
    "    return (df.shape)[0] != (df.drop_duplicates( subset=idx).shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "## this will have only contacts that are members, and only members that are contacts. \n",
    "## is possible to have duplicates if a contact is a member of more than one membership?\n",
    "## yes but the duplicated fields will only be in one group or the other\n",
    "## so we separate them and then reduce them\n",
    "## but we should really just make a function for this...\n",
    "\n",
    "## given two dataframes and two fields return two dataframes that are subsets of the original dataframes for which the two fields match\n",
    "def match_df(df1, df2, field1, field2):\n",
    "    df1 = df1[df1[field1].isin(df2[field2])]\n",
    "    df2 = df2[df2[field2].isin(df1[field1])]\n",
    "    return df1, df2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `test_lookup_fields`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# test that all external ids are unique\n",
    "# test that all lookups are valid\n",
    "\n",
    "def test_lookup_fields(df_d):\n",
    "    for fromKey in Salesforce.model_d.keys():\n",
    "        # verify that external id is unique\n",
    "        # assert df_d[fromKey][Salesforce.model_d[fromKey]['external_id']].is_unique, f\"external id not unique for {fromKey}\"\n",
    "\n",
    "        # verify that all lookups are valid \n",
    "        r_cols = [col for col in df_d[fromKey].columns if re.search('__r\\.', col)]\n",
    "        for col in r_cols:\n",
    "            col_matches = re.search('(.*)__r\\.(.*)', col)\n",
    "            fromField = col\n",
    "            lookupField = col_matches.group(1)+'__c'\n",
    "            toField = col_matches.group(2)\n",
    "            parentTable = Salesforce.model_d[fromKey]['lookups_d'][lookupField]\n",
    "            fromColumn = df_d[fromKey][fromField]\n",
    "            toColumn = df_d[parentTable][toField]\n",
    "            assert (fromColumn.isin(toColumn)).all(), f\"bad lookup: {fromKey} {fromField} {toField}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fields_pointing_to_foreign_key` \n",
    "`get_pointing_foreign_key_values` \n",
    "\n",
    "`reduce_to_referenced_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti    \n",
    "## these funcs all use the global variable df_d\n",
    "\n",
    "# function that returns all the fields that point to a given foreign key\n",
    "def fields_pointing_to_foreign_key(\n",
    "        foreign_key : str, \n",
    "        df_d : dict\n",
    "        ) -> list:\n",
    "    return_list = []\n",
    "    for name, df in df_d.items():\n",
    "        for col in df.columns:\n",
    "            if re.search(foreign_key[:-1], col) and  len(col)>len(foreign_key)+3:\n",
    "                return_list.append((name, col))\n",
    "    return return_list\n",
    "\n",
    "# function that takes a foreign key and returns a set of all value that point to it\n",
    "def get_pointing_foreign_key_values(\n",
    "    foreign_key: str, \n",
    "    df_d : dict\n",
    "    ) -> set:\n",
    "    return_set = set()\n",
    "    table_cols_l = fields_pointing_to_foreign_key(foreign_key, df_d)\n",
    "    for table, col in table_cols_l:\n",
    "        return_set.update(set(df_d[table][col]))\n",
    "    return return_set\n",
    "\n",
    "\n",
    "def reduce_to_referenced_rows(\n",
    "    df_d : dict\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    given a dictionary of dataframes, return a dictionary of dataframes where only rows whose foreign key is referenced by another table are kept\n",
    "    \"\"\"\n",
    "    # df2_d should be a dictionary of perfect dataframes\n",
    "    ## only rows whose foreign key is referenced by another table should be kept\n",
    "    df2_d = {}\n",
    "    for k, v in Salesforce.model_d.items():\n",
    "        o_df = df_d[k]\n",
    "\n",
    "        foreign_key = v['external_id']\n",
    "\n",
    "        # keep all rows if no other table points to this one\n",
    "        if len(fields_pointing_to_foreign_key(foreign_key, df_d)) == 0:\n",
    "            n_df = o_df\n",
    "            print(k,'*', len(o_df), len(n_df))\n",
    "            continue\n",
    "        point_to_foreign_key = get_pointing_foreign_key_values(foreign_key, df_d)\n",
    "        print(k, len(point_to_foreign_key)) \n",
    "        # good_keys = set(o_df[foreign_key]).intersection(point_to_foreign_key)\n",
    "        n_df = o_df.loc[ o_df[foreign_key].isin(point_to_foreign_key),:]\n",
    "        print(k,len(o_df),  len(n_df))\n",
    "        df2_d[k] = n_df\n",
    "    return df2_d\n",
    "\n",
    "# i want to write these to file and send them to salesforce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `perfect_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# starting from atms object dictionary, create a dictionary of dataframes for all SF objects\n",
    "# using this dictionary df_d, we can then remove duplicates of rows with same external_id\n",
    "# and remove any row which has a lookup to a non-existent foreign key\n",
    "\n",
    "@patch\n",
    "def perfect_data(self: Salesforce) -> dict:\n",
    "    assert len(self.atms.obj_d) == 4, 'atms dictionaries not available'\n",
    "    obj_l = Salesforce.model_d.keys()\n",
    "    for obj in obj_l:\n",
    "        # write atms dictionaries to csv file, if dictionary there - otherwise exception\n",
    "        self.process_objects(obj)\n",
    "\n",
    "    # create a dictionary of dataframes for all SF objects  \n",
    "    df_d = {}\n",
    "\n",
    "    # read csv files into dataframes\n",
    "    for i in Salesforce.model_d.keys(): \n",
    "        if os.path.isfile('sf_download/'+i+'.csv'):\n",
    "            df_d[i] = pd.read_csv('sf_download/'+i+'.csv', sep='\\t')\n",
    "    try:\n",
    "        # run test before cleaning (testing the test - it should fail)\n",
    "        print(\"this should fail\")\n",
    "        test_lookup_fields(df_d)    \n",
    "    except:\n",
    "        print(\"and it did. good.\")\n",
    "        \n",
    "    # for i in Salesforce.model_d.keys(): \n",
    "    for i in df_d.keys():\n",
    "        # remove duplicates of rows with same external_id\n",
    "        print(\"dropping duplicates for \", i,  )\n",
    "        # print(\"dropping duplicates for \", i, \" on \", self.model_d[i]['external_id'], len(df_d[i]))\n",
    "        df_d[i].drop_duplicates( inplace=True)\n",
    "        # df_d[i].drop_duplicates(subset= Salesforce.model_d[i]['external_id'], inplace=True)\n",
    "        print(\"AFTER dropping duplicates for \", i,  )\n",
    "        # print(\"AFTER dropping duplicates for \", i, \" on \", self.model_d[i]['external_id'], len(df_d[i]))\n",
    "\n",
    "    for obj,relations in self.model_d.items():\n",
    "        print(\"\\n ====================================\")\n",
    "        print(\"duplicate ext id loop: \", obj)\n",
    "        \n",
    "        if obj not in df_d.keys():\n",
    "            continue\n",
    "\n",
    "        for fromField, parent in relations['lookups_d'].items():\n",
    "            parentExternalId = Salesforce.model_d[parent]['external_id']\n",
    "            if parent in df_d.keys():   \n",
    "                toColumn = df_d[parent][parentExternalId]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # combine from field and parent external id to get Salesforce lookup field\n",
    "            newFromField = fromField[:-1]+'r.'+parentExternalId\n",
    "            \n",
    "            from_df, to_df = match_df(df_d[obj], df_d[parent], newFromField, parentExternalId)\n",
    "            print(f'{obj} --> {parent}  ', len(df_d[obj]), ' --> ', len(from_df))\n",
    "            # print(f'{parent}  ', len(df_d[parent]), ' --> ', len(to_df), '\\n')\n",
    "            df_d[obj] = from_df\n",
    "\n",
    "    try:\n",
    "        print(\"this should NOT fail\")\n",
    "        test_lookup_fields(df_d)\n",
    "    except:\n",
    "        print(\"but it did. Bad.\")\n",
    "        raise Exception(\"bad lookup\")\n",
    "    finally:\n",
    "        print(\"finally, it did not fail. Good.\")\n",
    "\n",
    "    ######### do we still need this ??? ############\n",
    "    df2_d = reduce_to_referenced_rows(df_d)\n",
    "    return df2_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `write_dict_to_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "@patch\n",
    "def write_dict_to_csv(\n",
    "    self: Salesforce,\n",
    "    df2_d : dict\n",
    "    ):\n",
    "    # write dictionary of dataframes to upload directory for all SF objects  \n",
    "    path = Salesforce.class_upload_dir\n",
    "    for k in Salesforce.model_d.keys(): \n",
    "        with open(path+k+'.csv', 'w') as f:\n",
    "            if k in df2_d:\n",
    "                f.write(df2_d[k].to_csv(sep='\\t', index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `upload_csv_to_sf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def upload_csv_to_sf(\n",
    "    self : Salesforce,\n",
    "    clean_sf : list | bool = [],\n",
    "    clean_all : bool = False,\n",
    "    operation : str = 'upsert'\n",
    "    ):\n",
    "\n",
    "    # clean SF if desired\n",
    "    if clean_all:\n",
    "        self.delete_sf_objects()\n",
    "    else:\n",
    "        for obj in clean_sf:\n",
    "            self.delete_sf_objects(obj)\n",
    "\n",
    "    # upload all data to SF\n",
    "    for obj,relations in Salesforce.model_d.items():\n",
    "        print(obj, relations['external_id'] )\n",
    "        self.execute_job(obj, 'upsert', external_id=relations['external_id'], use_ATMS_data=False) \n",
    "        sleep(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"\"\"{mermaid}\n",
    "graph TD;\n",
    "    Membership-->MemberTerm;\n",
    "    MemberTerm-->Member;\n",
    "    Users-->Member;\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "graph TD;\n",
    "    Membership-->MemberTerm;\n",
    "    MemberTerm-->Member;\n",
    "    Users-->Member;\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
